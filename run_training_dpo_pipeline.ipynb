{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Sdl9bZ1tFndH"
      },
      "source": [
        "# Training Pipeline\n",
        "[run_training_dpo_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "4VkhTAAYFndI"
      },
      "source": [
        "# Stage 1: Continue Pretraining\n",
        "\n",
        "第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文本数据上二次预训练GPT模型，以适配领域数据分布\n",
        "\n",
        "注意：\n",
        "1. 此阶段是可选的，如果你没有海量领域文本，可以跳过此阶段，直接进行SFT阶段的有监督微调\n",
        "2. 我实验发现：做领域知识注入，SFT比PT更高效，也可以跳过PT阶段\n",
        "\n",
        "| Stage 1: Continue Pretraining   |  [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py) | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPpbmkr4FndI"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B\n",
        "2. 数据集：PT阶段使用的是中文天龙八部小说部分文本和英文书籍部分文本，位于`data/pretrain`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "f7faCmbWFndJ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMuLFm3DFndJ"
      },
      "source": [
        "## 配置运行环境\n",
        "\n",
        "本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境\n",
        "\n",
        "colab建议使用T4 GPU训练，设置方式：`代码执行程序 -> 更改运行时类型 -> 运行时类型：Python3，硬件加速器：GPU，GPU类型：T4 -> 保存`\n",
        "\n",
        "步骤：\n",
        "1. 下载最新代码到本地\n",
        "2. 安装依赖包\n",
        "\n",
        "依赖包如下，保证最新版本：\n",
        "\n",
        "```\n",
        "loguru\n",
        "transformers\n",
        "sentencepiece\n",
        "datasets\n",
        "tensorboard\n",
        "tqdm\n",
        "peft\n",
        "trl\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OXU4igELJukd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNCsTFC5FndJ",
        "outputId": "d4526019-afad-44a1-e202-f32afd5a4ae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MedicalGPT'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 98 (delta 19), reused 52 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (98/98), 8.98 MiB | 28.46 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n",
            "/content/MedicalGPT\n",
            "build_domain_tokenizer.py   requirements.txt\n",
            "chatpdf.py                  reward_modeling.py\n",
            "CITATION.cff                \u001b[0m\u001b[01;34mrole_play_data\u001b[0m/\n",
            "_config.yml                 run_dpo.sh\n",
            "CONTRIBUTING.md             run_eval_quantize.sh\n",
            "convert_dataset.py          run_full_sft.sh\n",
            "\u001b[01;34mdata\u001b[0m/                       run_grpo.sh\n",
            "DISCLAIMER                  run_orpo.sh\n",
            "\u001b[01;34mdocs\u001b[0m/                       run_ppo.sh\n",
            "dpo_training.py             run_pt.sh\n",
            "eval_quantize.py            run_quant.sh\n",
            "fastapi_server_demo.py      run_rm.sh\n",
            "gradio_demo.py              run_sft_accelerate.sh\n",
            "grpo_training.py            run_sft.sh\n",
            "inference_multigpu_demo.py  run_training_dpo_pipeline.ipynb\n",
            "inference.py                run_training_ppo_pipeline.ipynb\n",
            "LICENSE                     supervised_finetuning_accelerate.py\n",
            "merge_peft_adapter.py       supervised_finetuning.py\n",
            "merge_tokenizers.py         template.py\n",
            "model_quant.py              validate_jsonl.py\n",
            "openai_api.py               vllm_deployment.sh\n",
            "orpo_training.py            zero1.yaml\n",
            "ppo_training.py             zero2.json\n",
            "pretraining.py              zero2.yaml\n",
            "README_EN.md                zero3.json\n",
            "README.md                   zero3.yaml\n"
          ]
        }
      ],
      "source": [
        "!git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
        "%cd MedicalGPT\n",
        "%ls\n",
        "#!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install specific compatible versions for the Model & Training\n",
        "!pip install transformers==4.46.3 peft==0.12.0 accelerate==0.34.2 trl==0.8.6 datasets"
      ],
      "metadata": {
        "id": "xviINhsCPGDu",
        "outputId": "6a3015f6-49a1-4f7d-daf1-f339a8667af3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.46.3\n",
            "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft==0.12.0\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting accelerate==0.34.2\n",
            "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting trl==0.8.6\n",
            "  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (2.32.4)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.46.3)\n",
            "  Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.46.3) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.12.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.12.0) (2.9.0+cu126)\n",
            "Collecting tyro>=0.5.11 (from trl==0.8.6)\n",
            "  Downloading tyro-1.0.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.3) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.46.3) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.12.0) (3.5.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.8.6) (0.17.0)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.8.6) (4.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.12.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.12.0) (3.0.3)\n",
            "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m140.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-1.0.1-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tyro, tokenizers, transformers, accelerate, trl, peft\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.3\n",
            "    Uninstalling transformers-4.57.3:\n",
            "      Successfully uninstalled transformers-4.57.3\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.12.0\n",
            "    Uninstalling accelerate-1.12.0:\n",
            "      Successfully uninstalled accelerate-1.12.0\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.18.0\n",
            "    Uninstalling peft-0.18.0:\n",
            "      Successfully uninstalled peft-0.18.0\n",
            "Successfully installed accelerate-0.34.2 peft-0.12.0 tokenizers-0.20.3 transformers-4.46.3 trl-0.8.6 tyro-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Install the Math dependencies (with the forced ANTLR runtime)\n",
        "!pip install latex2sympy2_extended math-verify==0.5.2 antlr4-python3-runtime==4.13.2"
      ],
      "metadata": {
        "id": "nTMKZ6t9PPuW",
        "outputId": "719d1a97-e6f5-4bf4-e4fe-c514f464696e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting latex2sympy2_extended\n",
            "  Downloading latex2sympy2_extended-1.10.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting math-verify==0.5.2\n",
            "  Downloading math_verify-0.5.2-py3-none-any.whl.metadata (347 bytes)\n",
            "Collecting antlr4-python3-runtime==4.13.2\n",
            "  Downloading antlr4_python3_runtime-4.13.2-py3-none-any.whl.metadata (304 bytes)\n",
            "Collecting latex2sympy2_extended\n",
            "  Downloading latex2sympy2_extended-1.0.6-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from latex2sympy2_extended) (1.14.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->latex2sympy2_extended) (1.3.0)\n",
            "Downloading math_verify-0.5.2-py3-none-any.whl (27 kB)\n",
            "Downloading latex2sympy2_extended-1.0.6-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading antlr4_python3_runtime-4.13.2-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: antlr4-python3-runtime, latex2sympy2_extended, math-verify\n",
            "  Attempting uninstall: antlr4-python3-runtime\n",
            "    Found existing installation: antlr4-python3-runtime 4.9.3\n",
            "    Uninstalling antlr4-python3-runtime-4.9.3:\n",
            "      Successfully uninstalled antlr4-python3-runtime-4.9.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "omegaconf 2.3.0 requires antlr4-python3-runtime==4.9.*, but you have antlr4-python3-runtime 4.13.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.13.2 latex2sympy2_extended-1.0.6 math-verify-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. (Optional) Install other utilities from the list\n",
        "!pip install loguru sentencepiece scikit-learn tensorboard tqdm"
      ],
      "metadata": {
        "id": "hZ__phJEPU3X",
        "outputId": "9b39a982-9705-4c71-850f-9c50ef6f1fdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting loguru\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.4)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru\n",
            "Successfully installed loguru-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Uninstall bitsandbytes to prevent Triton crashes (since you are using bf16, not 8-bit)\n",
        "!pip uninstall -y bitsandbytes"
      ],
      "metadata": {
        "id": "_EwOE1YpPXTK",
        "outputId": "1da8dfec-9d3f-4b8e-8906-1095749415e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxn6Js8hFndJ"
      },
      "source": [
        "## Stage1 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果\n",
        "\n",
        "**以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLFKpzpHFndJ",
        "outputId": "d66a5d00-9e68-46ed-9f1c-9b7c84d27c06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en_article_tail500.txt  fever.txt  tianlongbabu.txt\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/pretrain/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 100 ./data/pretrain/en_article_tail500.txt"
      ],
      "metadata": {
        "id": "2PLg1jrJKYIA",
        "outputId": "f286585e-721e-4e62-c737-147c020641c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "contract to work in specified mines and mills. There seemed to be no\n",
            "limit to the factories, forges, refineries, and railways that could be\n",
            "built, to the multitudes that could be employed in conquering a\n",
            "continent. As for the future, that was in the hands of Providence!\n",
            "\n",
            "=Business Theories of Politics.=--As the statesmen of Hamilton's school\n",
            "and the planters of Calhoun's had their theories of government and\n",
            "politics, so the leaders in business enterprise had theirs. It was\n",
            "simple and easily stated. \"It is the duty of the government,\" they\n",
            "urged, \"to protect American industry against foreign competition by\n",
            "means of high tariffs on imported goods, to aid railways by generous\n",
            "grants of land, to sell mineral and timber lands at low prices to\n",
            "energetic men ready to develop them, and then to leave the rest to the\n",
            "initiative and drive of individuals and companies.\" All government\n",
            "interference with the management, prices, rates, charges, and conduct of\n",
            "private business they held to be either wholly pernicious or intolerably\n",
            "impertinent. Judging from their speeches and writings, they conceived\n",
            "the nation as a great collection of individuals, companies, and labor\n",
            "unions all struggling for profits or high wages and held together by a\n",
            "government whose principal duty was to keep the peace among them and\n",
            "protect industry against the foreign manufacturer. Such was the\n",
            "political theory of business during the generation that followed the\n",
            "Civil War.\n",
            "\n",
            "\n",
            "THE SUPREMACY OF THE REPUBLICAN PARTY (1861-85)\n",
            "\n",
            "=Business Men and Republican Policies.=--Most of the leaders in industry\n",
            "gravitated to the Republican ranks. They worked in the North and the\n",
            "Republican party was essentially Northern. It was moreover--at least so\n",
            "far as the majority of its members were concerned--committed to\n",
            "protective tariffs, a sound monetary and banking system, the promotion\n",
            "of railways and industry by land grants, and the development of internal\n",
            "improvements. It was furthermore generous in its immigration policy. It\n",
            "proclaimed America to be an asylum for the oppressed of all countries\n",
            "and flung wide the doors for immigrants eager to fill the factories, man\n",
            "the mines, and settle upon Western lands. In a word the Republicans\n",
            "stood for all those specific measures which favored the enlargement and\n",
            "prosperity of business. At the same time they resisted government\n",
            "interference with private enterprise. They did not regulate railway\n",
            "rates, prosecute trusts for forming combinations, or prevent railway\n",
            "companies from giving lower rates to some shippers than to others. To\n",
            "sum it up, the political theories of the Republican party for three\n",
            "decades after the Civil War were the theories of American\n",
            "business--prosperous and profitable industries for the owners and \"the\n",
            "full dinner pail\" for the workmen. Naturally a large portion of those\n",
            "who flourished under its policies gave their support to it, voted for\n",
            "its candidates, and subscribed to its campaign funds.\n",
            "\n",
            "=Sources of Republican Strength in the North.=--The Republican party was\n",
            "in fact a political organization of singular power. It originated in a\n",
            "wave of moral enthusiasm, having attracted to itself, if not the\n",
            "abolitionists, certainly all those idealists, like James Russell Lowell\n",
            "and George William Curtis, who had opposed slavery when opposition was\n",
            "neither safe nor popular. To moral principles it added practical\n",
            "considerations. Business men had confidence in it. Workingmen, who\n",
            "longed for the independence of the farmer, owed to its indulgent land\n",
            "policy the opportunity of securing free homesteads in the West. The\n",
            "immigrant, landing penniless on these shores, as a result of the same\n",
            "beneficent system, often found himself in a little while with an estate\n",
            "as large as many a baronial domain in the Old World. Under a Republican\n",
            "administration, the union had been saved. To it the veterans of the war\n",
            "could turn with confidence for those rewards of service which the\n",
            "government could bestow: pensions surpassing in liberality anything that\n",
            "the world had ever seen. Under a Republican administration also the\n",
            "great debt had been created in the defense of the union, and to the\n",
            "Republican party every investor in government bonds could look for the\n",
            "full and honorable discharge of the interest and principal. The spoils\n",
            "system, inaugurated by Jacksonian Democracy, in turn placed all the\n",
            "federal offices in Republican hands, furnishing an army of party workers\n",
            "to be counted on for loyal service in every campaign.\n",
            "\n",
            "Of all these things Republican leaders made full and vigorous use,\n",
            "sometimes ascribing to the party, in accordance with ancient political\n",
            "usage, merits and achievements not wholly its own. Particularly was this\n",
            "true in the case of saving the union. \"When in the economy of\n",
            "Providence, this land was to be purged of human slavery ... the\n",
            "Republican party came into power,\" ran a declaration in one platform.\n",
            "\"The Republican party suppressed a gigantic rebellion, emancipated four\n",
            "million slaves, decreed the equal citizenship of all, and established\n",
            "universal suffrage,\" ran another. As for the aid rendered by the\n",
            "millions of Northern Democrats who stood by the union and the tens of\n",
            "thousands of them who actually fought in the union army, the Republicans\n",
            "in their zeal were inclined to be oblivious. They repeatedly charged the\n",
            "Democratic party \"with being the same in character and spirit as when it\n",
            "sympathized with treason.\"\n",
            "\n",
            "=Republican Control of the South.=--To the strength enjoyed in the\n",
            "North, the Republicans for a long time added the advantages that came\n",
            "from control over the former Confederate states where the newly\n",
            "enfranchised negroes, under white leadership, gave a grateful support to\n",
            "the party responsible for their freedom. In this branch of politics,\n",
            "motives were so mixed that no historian can hope to appraise them all at\n",
            "their proper values. On the one side of the ledger must be set the\n",
            "vigorous efforts of the honest and sincere friends of the freedmen to\n",
            "win for them complete civil and political equality, wiping out not only\n",
            "slavery but all its badges of misery and servitude. On the same side\n",
            "must be placed the labor of those who had valiantly fought in forum and\n",
            "field to save the union and who regarded continued Republican supremacy\n",
            "after the war as absolutely necessary to prevent the former leaders in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 100 ./data/pretrain/fever.txt"
      ],
      "metadata": {
        "id": "XNtEfW0yKfIP",
        "outputId": "d32318ab-8580-4170-b06a-eff3b4227a2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "第一章论\n",
            "传染病是指由病原微生物，如朊粒、病毒、衣原体、立克次体、支原体（mycoplasma)细菌真菌、螺旋体和寄生虫，如原虫、蠕虫、医学昆虫感染人体后产生的有传染性、在一定条件下可造成流行的疾病。感染性疾病是指由病原体感染所致的疾病，包括传染病和非传染性感染性疾病。\n",
            "传染病学是一门研究各种传染病在人体内外发生、发展、传播、诊断、治疗和预防规律的学科。重点研究各种传染病的发病机制、临床表现、诊断和治疗方法，同时兼顾流行病学和预防措施的研究，做到防治结合。\n",
            "传染病学与其他学科有密切联系，其基础学科和相关学科包括病原生物学、分子生物学、免疫学、人体寄生虫学、流行病学、病理学、药理学和诊断学等。掌握这些学科的基本知识、基本理论和基本技能对学好传染病学起着非常重要的作用。\n",
            "在人类历史长河中，传染病不仅威胁着人类的健康和生命，而且影响着人类文明的进程，甚至改写过人类历史。人类在与传染病较量过程中，取得了许多重大战果，19世纪以来，病原微生物的不断发现及其分子生物学的兴起，推动了生命科学乃至整个医学的发展；疫苗的研究诞生了感染免疫学，奠定了免疫学的理论基础，已用来研究各种疾病的发生机制及防治手段；抗生素的发现和应用被誉为20世纪最伟大的医学成就；“Koch法则“明确了传染病与病原微生物之间的因果关系，建立了病原学理论，已被广泛应用到其他许多疾病的研究，奠定了现代医学发展的基石。\n",
            "正是由于上述辉煌战果，加上社会文明的推进和物质生活水平的提高，人类逐渐在与传染病的斗争中占了上风。20世纪70年代西方医学界一度认为，传染病正在消亡。然而，1981年的艾滋病、2003年的传染性非典型肺炎、2012年的中东呼吸综合征、2013年的人感染H7N9禽流感、2014年的埃博拉出血热等新的传染病相继出现，不断给人类敲响警钟；与此同时，登革热、结核病、症疾及性传播疾病等老传染病再度肆虐，严重影响世界经济发展和社会和谐。20世纪90年代国际上提出了“eme1一ging infectiou s diseases\"的概念，起初被我国学者翻译为“新发传染病”，此后随着人们对感染性疾病认识的不断深入，该定义得到了修订，“新发传染病”逐渐演变为“新发感染病”，不仅包括由新种或新型病原微生物引起的新发现的感染病，而且包括近年来导致地区性或国际性公共卫生问题的再发的老感染病。新传染病的出现，老传染病的复燃，病原体对抗菌药物耐药性的增加，构成了对人类健康的巨大威胁。目前，世界卫生组织及各国政府均高度重视传染病防控工作，不断推出全球性的疾病诊断和指南，并使得传染病研究工作更容易得到跨地区、跨部门、跨领域的合作，研究成果也能更快地得到全球分享。\n",
            "新中国成立后，在“预防为主、防治结合”的卫生方针指引下，卫生条件明显改善，医药水平大幅提高，围生期保健工作不断加强，免疫接种覆盖率逐年提高，天花得到消灭，脊简灰质炎已接近被消灭，许多传染病如乙型脑炎、白喉、百日咳和新生儿破伤风等的发病率也明显下降。但是有些传染病如病毒性肝炎、出血热、狂犬病、结核病和感染性腹泻等仍然广泛存在；新发感染病包括变异病原体感染多次出现流行，如传染性非典型肺炎及甲型H1Nl流感的肆虐，国外流行的传染病亦有可能传入我国；烈性传染病还有可能成为生物恐怖的主要工具。因此，对传染病的防治研究仍需加强。传染病研究一直是国家重大科研项目和药物开发的重点领域，是当前国家重大科技需求。\n",
            "2第一章总论\n",
            "祖国医学对传染病的防治有丰富的经验，深入发掘和发展祖国医学研究将对中西结合防治传染病发挥重要作用。\n",
            "第一节感染与免疫—、感染的概念\n",
            "感染是病原体和人体之间相互作用、相互斗争的过程。引起感染的病原体有500种以上，可来自宿主体外，也可来自宿主体内（包括在黏膜腔内移行移位或潜伏在组织器官）。来自宿主体外病原体引起的感染称为传染，传染主要指病原体通过一定方式从一个宿主个体到另一个宿主个体的感染。构成传染和感染过程必须具备三个因素，即病原体、人体和它们所处的环境，三者之间此消彼长。在漫长的生物进化过程中，病原体与宿主形成了相互依存、相互斗争的关系。有些微生物、寄生虫与人体宿主之间达到了互相适应、互不损害对方的共生状态，如肠道中的大肠埃希菌和某些真菌。但是，这种平衡是相对的，当某些因素导致宿主的免疫功能受损（如应用大剂量皮质激素或抗肿瘤药物、放射治疗及艾滋病等），或大量应用抗菌药物引起的菌群失调症，或机械损伤使寄生物离开其固有的寄生部位而到达其他寄生部位（如大肠埃希菌进入泌尿道或呼吸道），平衡就不复存在而引起宿主损伤，这种情况称为机会性感染。这些共生菌在特定条件下可以成为致病菌，称为条件致病菌。在病原体与宿主的相互斗争过程中，宿主逐步形成了特异的免疫防御机制。\n",
            "20世纪70年代以来，相继出现一些新的病原体，如人免疫缺陷病毒、新型冠状病毒、H7N9禽流感病毒、新型布尼亚病毒等，分别引起艾滋病、传染性非典型肺炎、中东呼吸综合征、人感染H7N9禽流感、发热伴血小板减少综合征等新发感染病；一些已经被控制的传染病，如性病、登革热、结核病等，由千种种原因又在局部地区流行。广谱抗生素的滥用诱发葡萄球菌、部分肠杆菌科细菌等病原菌发生耐药基因突变，引起难治性耐药菌株感染。\n",
            "临床上可碰到多种形式的感染情况。人体初次被某种病原体感染称为首发感染。有些传染病很少出现再次感染，如麻疹、水疫、流行性腮腺炎等。人体在被某种病原体感染的基础上再次被同一种病原体感染称为重复感染，较常见千疤疾、血吸虫病和钩虫病等。人体同时被两种或两种以上的病原体感染称为混合感染，这种情况临床上较为少见。人体在某种病原体感染的基础上再被另外的病原体感染称为蜇叠感染，这种情况临床上较为多见，如慢性乙型肝炎病毒感染重叠戊型肝炎病毒感染。在重叠感染中，发生千原发感染后的其他病原体感染称为继发性感染，如病毒性肝炎继发细菌、真菌感染。此外，住院患者在医院内获得的感染称为医院获得性感染，即医院感染，这类感染的来源不同，有医院内通过患者或医护人员直接或间接传播引起的交叉感染、患者自己体内正常菌群引发的自身感染或内源性感染以及诊疗过程中或因医疗器械消毒不严而造成的医源性感染等。医院感染包括在住院期间发生的感染和在医院内获得但在出院后发生的感染，但不包括入院前已开始或入院时已存在的感染，后者称为社区获得性感染，指的是在医院外罹患的感染，包括具有明确潜伏期而在入院后平均潜伏期内发病的感染。\n",
            "二、感染过程的表现\n",
            "病原体通过各种途径进入人体后就开始了感染的过程。在一定的环境条件影响下，根据入体防御功能的强弱和病原体数量及毒力的强弱，感染过程可以出现五种不同的结局，即感染谱。这些表现可以移行或转化，呈现动态变化。\n",
            "（－）病原体被清除\n",
            "病原体被清除是通过以下两种机制来实现的。病原体进入人体后，首先可被机体非特异性防御能力所清除，这种防御能力有皮肤和黏膜的屏障作用、胃酸的杀菌作用、正常体液的溶菌作用、组织内细胞的吞噬作用等。这些综合性的能力就是所谓人体的非特异性免疫，是人类在长期进化过程中，不断与病原生物斗争而逐渐形成的，并可遗传给后代。同时，亦可由事先存在于体内的特异性体液免疫与细胞免疫物质（特异性免疫球蛋白与细胞因子）将相应的病原体清除。\n",
            "（二）隐性感染\n",
            "隐性感染又称亚临床感染，是指病原体侵入人体后，仅诱导机体产生特异性免疫应答，而不引起或只引起轻微的组织损伤，因而在临床上不显出任何症状、体征甚至生化改变，只能通过免疫学检查才能发现。在大多数病毒性传染病中（如脊髓灰质炎和流行性乙型脑炎），隐性感染是最常见的表现，其数量常远远超过显性感染(10倍以上）。隐性感染过程结束以后，大多数人获得不同程度的特异性免疫，病原体被清除。少数人可转变为病原携带状态，病原体持续存在于体内，成为无症状携带者，如伤寒沙门菌、志贺菌和乙型肝炎病毒感染等。隐性感染在传染病流行期间，对防止流行的扩散有积极意义，因为隐性感染者的增多，人群对某一种传染病的易感性就降低，该种传染病的发病率就下降。但另—方面，隐性感染者也可能处千病原携带状态，在传染病流行期间成为重要的传染源。\n",
            "（三）显性感染\n",
            "显性感染又称临床感染，是指病原体侵入人体后，不但诱导机体发生免疫应答，而且，通过病原体本身的作用或机体的变态反应，导致组织损伤，引起病理改变和临床表现。在大多数传染病中，显性感染只占全部受感染者的小部分，好比海上冰山露出水面的一个小尖峰。但在少数传染病中，如麻疹、水痉等，大多数感染者表现为显性感染。在同一种传染病，由于病原体致病力与人体抗病能力的差异，显性过程又可呈现轻、重型，与急、慢性等各种类型。有些传染病在显性感染过程结束后，病原体可被清除，感染者可获得较为稳固的免疫力，如麻疹、甲型肝炎和伤寒等，不易再受感染。但另有一些传染病病后的免疫力并不牢固，可以再受感染而发病，如细菌性痢疾、阿米巴痢疾等。小部分显性感染者亦可成为慢性病原携带者。\n",
            "（四）病原携带状态\n",
            "病原携带状态是指病原体侵入人体后，可以停留在入侵部位，或侵入较远的脏器继续生长、繁殖，而人体不出现任何的疾病状态，但能携带并排除病原体，成为传染病流行的传染源。这是在传染过程中人体防御能力与病原体处于相持状态的表现。按病原体的种类不同，病原携带者可分为带病毒者、带菌者或带虫者等C按其发生和持续时间的长短可分为潜伏期携带者、恢复期携带者或慢性携带者。一般而言，若其携带病原体的持续时间短于3个月，称为急性携带者；若长于3个月，则称为慢性携带者。对乙型肝炎病毒感染，超过6个月才算慢性携带者。所有病原携带者都有一个共同的特点，即无明显临床症状而携带病原体，因而，在许多传染病中，如伤寒、细菌性痢疾、霍乱、白喉、流行性脑脊髓膜炎和乙型肝炎等，成为重要的传染源。但并非所有传染病都有慢性病原携带者，如恙虫病、甲型病毒性肝炎、登革热和流行性感冒等，慢性病原携带者极为罕见。\n",
            "（五）潜伏性感染\n",
            "潜伏性感染又称潜在性感染。病原体感染人体后，寄生于某些部位，由于机体免疫功能足以将病原体局限化而不引起显性感染，但又不足以将病原体清除时，病原体便可长期潜伏起来，待机体免疫功能下降时，则可引起显性感染。常见的潜伏性感染有单纯疤疹病毒、水瘦病毒、疤原虫和结核分枝杆菌等感染。潜伏性感染期间，病原体一般不排出体外，这是与病原携带状态不同之处。潜伏性感染并不是在每种传染病中都存在。\n",
            "除病原体被清除以外，另外四种表现形式在不同传染病中各有侧重，一般来说，隐性感染最常见，病原携带状态次之，显性感染所占比重最低，但一旦出现，则容易识别。而且，上述感染的五种表现形＼配fii式不是一成不变的，在一定条件下可相互转变，同一种疾病的不同阶段可以有不同的表现形式。\n",
            "三、感染过程中病原体的作用\n",
            "病原体侵入人体后能否引起疾病，取决于病原体的致病能力和机体的免疫功能这两方面因素。致病能力包括以下几方面：侵袭力是指病原体侵入机体并在机体内生长、繁殖的能力。有些病原体可直接侵入人体，如钩端螺旋体、钩虫丝状蚴和血吸虫尾蚴等。有些病原体则需经消化道或呼吸道进入人体，先黏附于肠或支气管黏膜表面，再进一步侵入组织细胞，产生毒素，引起病变，如志贺菌、结核分枝杆菌等。病毒性病原体常通过与细胞表面的受体结合再进入细胞内。有些细菌的表面成分（如伤寒沙门菌的V}抗原）有抑制吞噬作用的能力而促进病原体的扩散。引起腹泻的大肠埃希菌能表达受体和小肠细胞结合，称为定植因子。有些病原体的侵袭力较弱，需经伤口进入人体，如破伤风杆菌、狂犬病病毒等。\n",
            "（二）毒力\n",
            "毒力包括毒素和其他毒力因子。毒素包括外毒素与内毒素。前者以白喉杆菌、破伤风杆菌和霍乱弧菌为代表。后者以伤寒沙门菌、志贺菌为代表。外毒素通过与靶细胞的受体结合，进入细胞内而起作用。内毒素则通过激活单核－吞噬细胞、释放细胞因子而起作用。其他毒力因子有：穿透能力（钩虫丝状蚴）、侵袭能力（志贺菌）、溶组织能力（溶组织内阿米巴）等。许多细菌都能分泌抑制其他细菌生长的细菌素以利于本身生长、繁殖。\n",
            "（三）数量\n",
            "在同一种传染病中，入侵病原体的数量一般与致病能力成正比。然而，在不同的传染病中，能引起疾病的最低病原体数量可有较大差异，如伤寒需要10万个菌体，而细菌性痢疾仅为10个菌体。\n",
            "（四）变异性\n",
            "病原体可因环境、药物或遗传等因素而发生变异。一般来说，在人工培养多次传代的环境下，可使病原体的致病力减弱，如用于结核病预防的卡介苗；在宿主之间反复传播可使致病力增强，如肺鼠疫。病原体的抗原变异可逃逸机体的特异性免疫作用而继续引起疾病或使疾病慢性化，如流行性感冒病毒、丙型肝炎病毒和人免疫缺陷病毒等。\n",
            "四、感染过程中免疫应答的作用\n",
            "机体的免疫应答对感染过程的表现和转归起着重要的作用。免疫应答可分为有利于机体抵抗病原体的保护性免疫应答和促进病理改变的变态反应两大类。保护性免疫应答又分为非特异性免疫应答和特异性免疫应答两类，都有可能引起机体保护和病理损伤。变态反应都是特异性免疫应答。\n",
            "（一）非特异性免疫\n",
            "非特异性免疫是机体对侵入病原体的一种清除机制。它不牵涉对抗原的识别和二次免疫应答的增强。\n",
            "1天然屏障包括外部屏障，即皮肤、黏膜及其分泌物，如溶菌酶、气管黏膜上的纤毛等；以及内部屏障，如血－脑屏障和胎盘屏障等。\n",
            "2.吞噬作用单核－吞噬细胞系统包括血液中的游走大单核细胞，肝、脾、淋巴结、骨髓中固有的吞噬细胞和各种粒细胞（尤其是中性粒细胞）。它们都具有非特异性吞噬功能，可清除机体内的病原体。\n",
            "3体液因子包括存在于体液中的补体、溶菌酶(lysozyme汃纤连蛋白(fibronectin汃各种细胞因子和细胞激素样肤类物质等。细胞因子主要是由单核－吞噬细胞和淋巴细胞被激活后释放的一类有生物活性的肤类物质。这些体液因子能直接或通过免疫调节作用而清除病原体。与非特异性免疫应答有关的细胞因子有白细胞介素、Q－肿瘤坏死因子(tumor necrosis factor-ex, TNF飞）、丫－干扰素、粒细胞－巨噬细胞集落刺激因子等。\n",
            "（二）特异性免疫\n",
            "特异性免疫是指由于对抗原特异性识别而产生的免疫。由于不同病原体所具有的抗原绝大多数是不相同的，故特异性免疫通常只针对一种病原体。感染后免疫都是特异性免疫，而且是主动免疫，通过细胞免疫和体液免疫的相互作用而产生免疫应答，分别由T淋巴细胞与B淋巴细胞介导。\n",
            "1细胞免疫致敏T细胞与相应抗原再次相遇时，通过细胞毒性淋巴因子来杀伤病原体及其所寄生的细胞。对细胞内寄生病原体的清除作用，细胞免疫起重要作用。T细胞还具有调节体液免疫的功能。\n",
            "2.体液免疫致敏B细胞受抗原刺激后，即转化为浆细胞并产生能与相应抗原结合的抗体，即免疫球蛋白。不同的抗原可诱发不同的免疫应答，因而抗体又可分为抗毒素、抗菌性抗体、中和抗体及调理素等，可促进细胞吞噬功能、清除病原体。抗体主要作用千细胞外的微生物。在化学结构上lg可分为5类，即IgG、IgA、IgM、JgD和IgE，各具不同功能。在感染过程中IgM首先出现，但持续时间不长，是近期感染的标志。lgG随后出现，并持续较长时期。IgA主要是呼吸道和消化道黏膜上的局部抗体。IgE则主要作用千入侵的原虫和蠕虫。\n",
            "（李兰娟）\n",
            "第二节传染病的发病机制\n",
            "—、传染病的发生与发展\n",
            "传染病的发生与发展都有一个共同的特征，就是疾病发展的阶段性。发病机制中的阶段性与临床表现的阶段性大多数是互相吻合的，但有时并不完全一致，例如，在伤寒第一次菌血症时还未出现症状，第4周体温下降时肠壁溃疡尚未完全愈合。\n",
            "入侵部位\n",
            "病原体的入侵部位与发病机制有密切关系，入侵部位适当，病原体才能定植、生长、繁殖及引起病变。如志贺菌和雀乱弧菌都必须经口感染，破伤风杆菌必须经伤口感染，才能引起病变。\n",
            "（二）机体内定位\n",
            "病原体入侵并定植后，可在入侵部位直接引起病变，如恙虫病的焦痐；也可在入侵部位繁殖，分泌毒素，在远离入侵部位引起病变，如白喉和破伤风；也可进入血液循环，再定位于某一脏器（靶器官）引起该器官的病变，如流行性脑脊髓膜炎和病毒性肝炎；还可经过一系列的生活史阶段，最后在某脏器中定居，如蠕虫病。各种病原体的机体内定位不同，各种传染病都有其各自的特殊规律性。\n",
            "（三）排出途径\n",
            "各种传染病都有其病原体排出途径，是患者、病原携带者和隐性感染者有传染性的重要因素。有些病原体的排出途径是单一的，如志贺菌只通过粪便排出；有些病原体可有多种排出途径，如脊髓灰质炎病毒既可通过粪便排出又可通过飞沫排出；有些病原体则存在于血液中，当虫媒叮咬或输血时才离开人体（如疤原虫）。病原体排出体外的持续时间有长有短，因而，不同传染病组织损伤及功能受损是疾病发生的基础。在传染病中，导致组织损伤的发生方式有下列三种：直接损伤病原体借助其机械运动及所分泌的酶可直接破坏组织（如溶组织内阿米巴滋养体），或通过细胞病变而使细胞溶解（如脊髓灰质炎病毒），或通过诱发炎症过程而引起组织坏死（如鼠疫）。（二）毒素作用有些病原体能分泌毒力很强的外毒素，可选择性损害靶器官（如肉毒杆菌的神经毒素）或引起功能紊乱（如霍乱肠毒素）。革兰阴性杆菌裂解后产生的内毒素则可激活单核－吞噬细胞分泌TNF-a和其他细胞因子，导致发热、休克及弥散性血管内凝血等现象。\n",
            "（三）免疫机制\n",
            "许多传染病的发病机制与免疫应答有关。有些传染病能抑制细胞免疫（如麻疹）或直接破坏T细胞（如艾滋病），更多的病原体则通过变态反应而导致组织损伤，其中，以III型（免疫复合物）反应（如肾综合征出血热）及W型（细胞介导）反应（如结核病及血吸虫病）最为常见。\n",
            "三、重要的病理生理变化\n",
            "发热\n",
            "发热常见千传染病，但并非传染病所特有。外源性致热原（病原体及其产物、免疫复合物、异性蛋白、大分子化合物或药物等）进入人体后，激活单核－吞噬细胞、内皮细胞和B淋巴细胞等，使后者释放内源性致热原，如白细胞介素－1、TNF、IL-6和干扰素等。内源性致热原通过血液循环刺激体温调节中枢，释放前列腺素E2。后者把恒温点调高，使产热超过散热而引起体温上升。\n",
            "（二）急性期改变感染、创伤、炎症等过程所引起的一系列急性期机体应答称为急性期改变。它出现千感染发生后几小时至几天。主要的改变如下：1.蛋白代谢肝脏合成一系列急性期蛋白，其中C反应蛋白是急性感染的重要标志。红细胞沉降率加快也是血浆内急性期蛋白浓度增高的结果。由于糖原异生作用加速，能量消耗，肌肉蛋白分解增多，进食减少等均可导致负氮平衡与消瘦。\n",
            "2.糖代谢葡萄糖生成加速，导致血糖升高，糖耐量短暂下降，这与糖原异生作用加速及内分泌影响有关。在新生儿及营养不良的想者，或肝衰竭患者，糖原异生作用也可下降导致血糖下降。\n",
            "3.水电解质代谢急性感染时，氯化钠因出汗、呕吐或腹泻而丢失，加上抗利尿素分泌增加、尿最减少、水分游留而导致低钠血症，至恢复期才出现利尿。由千钾的摄入减少和排出增加而导致钾的负平衡。吞噬细胞被激活后释出的介质则导致铁和锌由血浆进入单核－吞噬细胞系统，故持续感染可导致贫血。由千铜蓝蛋白分泌增多可导致高铜血症。\n",
            "4内分泌改变在急性感染早期，随着发热开始，由促肾上腺皮质激素所介导的糖皮质激素和类固醇在血中浓度升高，其中糖皮质激素水平可高达正常的5倍。但在败血症并发肾上腺出血时则可导致糖皮质激素分泌不足或停止。酪固酮分泌增加可导致氯和钠的涨留。中枢神经系统感染引起的抗利尿激素分泌增加可导致水分渚留。在急性感染早期，胰高血糖素和胰岛素的分泌有所增加，血中甲状腺素水平在感染早期因消耗增多而下降，后期随着垂体反应刺激甲状腺素分泌而升高。\n",
            "（李兰娟）\n",
            "第一章总论\n",
            "传染病的流行过程就是传染病在入群中发生、发展和转归的过程。流行过程的发生需要有三个基本条件，包括传染源、传播途径和人群易感性。这三个环节必须同时存在，若切断任何一个环节，流行即告终止。流行过程本身又受自然因素、社会因素和个人行为因素的影响。\n",
            "—、流行过程的基本条件\n",
            "（一）传染源传染源是指体内有病原体生存、繁殖并能将病原体排出体外的人和动物。传染源包括下列四个方面：1患者是大多数传染病重要的传染源。不同病期的患者其传染强度可有不同，一般情况下，以发病早期的传染性最大。慢性感染患者可长期排出病原体，可成为长期传染源。2隐性感染者在某些传染病中，如流行性脑脊髓膜炎、脊髓灰质炎等，隐性感染者在病原体被清除前是重要的传染源。3病原携带者慢性病原携带者无明显临床症状而长期排出病原体，在某些传染病中，如伤寒、细菌性痢疾等，有重要的流行病学意义。\n",
            "4.受感染的动物以啃齿动物最为常见，其次是家畜、家禽。这些以动物为传染源传播的疾病，称为动物源性传染病。有些动物本身发病，如鼠疫、狂犬病、布鲁菌病等；有些动物不发病，表现为病原携带状态，如地方性斑疹伤寒、恙虫病、流行性乙型脑炎等。以野生动物为传染源传播的疾病，称为自然疫源型传染病，如鼠疫、钩端螺旋体病、肾综合征出血热、森林脑炎等。由于动物传染源受地理、气候等自然因素的影响较大，动物源性传染病常存在于一些特定的地区，并具有严格的季节性。\n",
            "（二）传播途径病原体离开传染源到达另一个易感者的途径称为传播途径，同一种传染病可以有多种传播途径。1呼吸道传播病原体存在千空气中的飞沫或气溶胶中，易感者吸入时获得感染，如麻疹、白喉、结核病、禽流感和严重急性呼吸综合征等。\n",
            "2.消化道传播病原体污染食物、水源或食具，易感者于进食时获得感染，如伤寒、细菌性痢疾和霍乱等。\n",
            "3接触传播易感者与被病原体污染的水或土壤接触时获得感染，如钩端螺旋体病、血吸虫病和钩虫病等。伤口被污染，有可能患破伤风。日常生活的密切接触也有可能获得感染，如麻疹、白喉、流行性感冒等。不洁性接触（包括同性恋、多个性伴侣的异性恋及商业性行为）可传播HIV、HBV、HCV、梅毒螺旋体、淋病奈瑟菌等。\n",
            "4.虫媒传播被病原体感染的吸血节肢动物，如按蚊、人乱、鼠圣、白蛉、硬蟀和恙蜡等，于叮咬时把病原体传给易感者，可分别引起疤疾、流行性斑疹伤寒、地方性斑疹伤寒、黑热病、莱姆病和恙虫病等。根据节肢动物的生活习性，往往有严格的季节性，有些病例还与感染者的职业及地区相关。\n",
            "上述途径传播统称为水平传播，母婴传播属于垂直传播。婴儿出生前已从母亲或父亲获得的感染称为先天性感染，如梅毒、弓形虫病。（三）人群易感性对某种传染病缺乏特异性免疫力的人称为易感者，易感者在某一特定人群中的比例决定该人群的易感性。当易感者在某一特定人群中的比例达到一定水平，若又有传染源和合适的传播途径时，则很容易发生该传染病流行。某些病后免疫力很巩固的传染病（如麻疹、水症、乙型脑炎），经过一次流行之后，需待几年当易感者比例再次上升至一定水平时，才会发生另一次流行。这种现象称为传染病流行的周期性。在普遍推行人工主动免疫的情况下，可把某种传染病的易感者水平始终保持很低，从而阻止其流行周期性的发生。\n",
            "二、影响流行过程的因素\n",
            "自然因素\n",
            "自然环境中的各种因素，包括地理、气象和生态等，对传染病流行过程的发生和发展都有重要影响。寄生虫病和由虫媒传播的传染病对自然条件的依赖性尤为明显。传染病的地区性和季节性与自然因素有密切关系，如我国北方有黑热病地方性流行区，南方有血吸虫病地方性流行区，症疾、乙型脑炎的夏秋季发病率较高等都与自然因素有关。自然因素可直接影响病原体在外环境中的生存能力，如钩虫病少见于干旱地区。自然因素也可通过降低机体的非特异性免疫力而促进流行过程的发展，如寒冷可减弱呼吸道抵抗力，炎热可减少胃酸的分泌等。某些自然生态环境为传染病在野生动物之间的传播创造了良好条件，如鼠疫、恙虫病和钩端螺旋体病等，人类进入这些地区时亦可受感染，称为自然疫源型传染病或人兽共患病。\n",
            "（二）社会因素\n",
            "社会因素包括社会制度、经济状况、生活条件和文化水平等，对传染病流行过程有重大影响。新中国成立后，社会制度使人民生活、文化水平不断提高，施行计划免疫，已使许多传染病的发病率明显下降或接近被消灭。由于改革开放、市场化经济政策的实施，在国民经济日益提高的同时，因人口流动、生活方式、饮食习惯的改变和环境污染等，有可能使某些传染病的发病率升高，如结核病、艾滋病、并殖吸虫病和症疾等。这应引起我们的重视。\n",
            "（三）个人行为因素\n",
            "人类自身不文明、不科学的行为和生活习惯，也有可能造成传染病的发生与传播，这些行为和习惯往往体现在旅游、打猎、集会、日常生活、豢养宠物等过程中。因此，个人旅游应有的防病准备、公共场合的卫生防范、居家卫生措施、自身健康教育均显示其重要性。\n",
            "（李兰娟）\n",
            "第四节传染病的特征—、基本特征\n",
            "传染病与其他疾病的主要区别在于其具有下列四个基本特征：病原体每种传染病都是由特异性病原体引起的。病原体可以是微生物或寄生虫。近年还证实一种不同于微生物和寄生虫，缺乏核酸结构的具有感染性的变异蛋白质，称为朊粒，是入类几种中枢神经系统退行性疾病一克－雅病、库鲁病及变异性克－雅病即人类疯牛病等的病原。历史上许多传染病都是先认识其临床和流行病学特征，然后才认识其病原体。随着研究水平的不断提高和深入，对各种传染病病原体的认识也逐渐加深。特定病原体的检出在确定传染病的诊断和流行中有着重大意义。由于新技术的应用，有彸如，／可能发现新的传染病病原体。\n",
            "第—章总论\n",
            "（二）传染性\n",
            "传染性是传染病与其他感染性疾病的主要区别。例如，耳源性脑膜炎和流行性脑脊髓膜炎，在临床上都表现为化脓性脑膜炎，但前者无传染性，无须隔离，后者则有传染性，必须隔离。传染性意味着病原体能通过某种途径感染他人。传染病患者有传染性的时期称为传染期。它在每一种传染病中都相对固定，可作为隔离患者的依据之一。\n",
            "（三）流行病学特征\n",
            "传染病的流行过程在自然和社会因素的影响下，表现出各种流行病学特征:1.流行性可分为散发、暴发、流行和大流行。散发是指某传染病在某地的常年发病情况处千常年一般发病率水平，可能是由于人群对某病的免疫水平较高，或某病的隐性感染率较高，或某病不容易传播等。暴发是指在某一局部地区或集体单位中，短期内突然出现许多同一疾病的患者，大多是同一传染源或同一传播途径，如食物中毒、流行性感冒等。当某病发病率显著超过该病常年发病率水平或为散发发病率的数倍时称为流行。当某病在一定时间内迅速传播，波及全国各地，甚至超出国界或洲境时称为大流行或称世界性流行，如2003年的传染性非典型肺炎大流行、2009年的甲型HlNI流感大流行。\n",
            "2.季节性不少传染病的发病率每年都有一定的季节性升高，主要原因为气温的高低和昆虫媒介的有无。如呼吸道传染病常发生在寒冷的冬、春季节，肠道传染病及虫媒传染病好发千炎热的夏、秋季节。\n",
            "3地方性有些传染病或寄生虫病由于中间宿主的存在、地理条件、气温条件、人民生活习惯等原因，常局限在一定的地理范围内发生，如恙虫病、疤疾、血吸虫病、丝虫病、黑热病等。主要以野生动物为传染源的自然疫源性疾病也属千地方性传染病。\n",
            "4.外来性指在国内或地区内原来不存在，而从国外或外地通过外来人口或物品传入的传染病，如霍乱。\n",
            "（四）感染后免疫\n",
            "感染后免疫指免疫功能正常的人体经显性或隐性感染某种病原体后，都能产生针对该病原体及其产物（如毒素）的特异性免疫。通过血清中特异性抗体的检测可知其是否具有免疫力。感染后获得的免疫力和疫苗接种一样都属千主动免疫。通过注射或从母体获得抗体的免疫力都属千被动免疫。感染后免疫力的持续时间在不同传染病中有很大差异。有些传染病，如麻疹、脊韵灰质炎和乙型脑炎等，感染后免疫力持续时间较长，甚至保持终生；但有些传染病则感染后免疫力持续时间较短，如流行性感冒、细菌性痢疾和阿米巴病等。在临床上，感染后免疫如果持续时间较短，可出现下列现象：1再感染指同一传染病在痊愈后，经过长短不等间隙再度感染，如感冒、细菌性痢疾。2重复感染指疾病尚在进行过程中，同一种病原体再度侵袭而又感染，此在蠕虫病（如血吸虫病、并殖吸虫病、丝虫病）中较为常见，是发展为重症的主要原因，因其感染后通常不产生保护性免疫。\n",
            "二、临床特点\n",
            "（一）病程发展的阶段性\n",
            "急性传染病的发生、发展和转归，通常分为以下四个阶段：1.潜伏期从病原体侵入人体起，至开始出现临床症状为止的时期，称为潜伏期。每一个传染病的潜伏期都有一个范围（最短、最长），并呈常态分布，是检疫工作观察、留验接触者的重要依据。潜伏期相当千病原体在体内定位、繁殖和转移、引起组织损伤和功能改变导致临床症状出现之前的整个过程，其长短不一，随病原体的种类、数量、毒力与人体免疫力的强弱而定，短的仅数小时（如细菌性食物中毒），大多数在数天内（如白喉、猩红热、细菌性痢疾等），有的可延至数月（如狂犬病）甚或数年以上（如麻风、艾滋病）。潜伏期的长短通常与病原体的感染址成反比。如果主要由毒素引起病理生理改变，则与毒素产生和播散所需时间有关。如细菌性食物中毒，毒素在食物中已预先存在，则潜伏期可短至数十分钟。狂犬病的潜伏期取决千狂犬病毒进入人体的部位（伤口），离中枢神经系统越近则潜伏期越短。在蠕虫病，由于幼虫的移行，在潜伏期即可出现症状，因此潜伏期的计算应自病原体入侵人体至虫卵或幼虫出现为止这一阶段，通常较细菌性疾病的潜伏期要长得多（大多数在数月以上），如血吸虫病、丝虫病、并殖吸虫病等。潜伏期短的传染病，流行时往往呈暴发。有些传染病在潜伏期末已具传染性。\n",
            "2.前驱期从起病至症状明显开始为止的时期称为前驱期。在前驱期中的临床表现通常是非特异性的，如头痛、发热、疲乏、食欲下降和肌肉酸痛等，与病原体繁殖产生的毒性物质有关，为许多传染病所共有，一般持续l~3天。前驱期已具有传染性。起病急骤者，可无前驱期。\n",
            "3症状明显期急性传染病患者度过前驱期后，某些传染病，如麻疹、水疫患者往往转入症状明显期。在此期间该传染病所特有的症状和体征都通常获得充分的表现，如具有特征性的皮疹、黄疽、肝脾肿大和脑膜刺激征等。然而，在某些传染病，如脊髓灰质炎、乙型脑炎等，大部分患者可随即进入恢复期，临床上称为顿挫型，仅少部分患者进入症状明显期。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 100 ./data/pretrain/tianlongbabu.txt"
      ],
      "metadata": {
        "id": "mDj0h4DFKlGp",
        "outputId": "9a73e6c4-c865-4e22-cce4-f69796a5ea6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "天龙八部\r\n",
            "\r\n",
            "\r\n",
            "正文 释名\r\n",
            "“天龙八部”这名词出于佛经。许多大乘佛经叙述佛向诸菩萨、比丘等说法时，崐常有天龙八部参与听法。如“法华经：提婆达多品”：“天龙八部、人与非人，皆崐遥见彼龙女成佛”。\r\n",
            "“非人”，包括八种神道怪物，因为以“天”及“龙”为首，崐所以称为《天龙八部》。八部罗，七归那罗，八摩听罗迦。\r\n",
            "“天”是指天神。在佛教中，天神的地位并非至高无上，只不过比人能享受到崐到更大、更长久的福报而已。佛教认为一切事物无常，天神的寿命终了之后，也是崐要死的。天神临死之前有五种征状：衣裳垢腻、头上花萎、身体臭秽、腋下汗出、崐不乐本座(第五个征状或说是“玉子离散”)，这就是所谓“天人五衰”，是天神最崐大的悲哀。帝释是众天神的领袖。\r\n",
            "“龙”是指神。佛经中的龙，和我国的传说中的龙大致差不多，不过没有脚，崐有的大蟒蛇也称。事实上，中国人对龙和龙王的观念，主要是从佛经中来的。佛经崐中有五龙五、七龙王、八龙王等等名称，古印度人龙很是尊敬，认为水中主物以龙崐的力气最大，因此对德行崇高的人尊称为“龙象”，如西来龙”，那是指从西方来崐的高僧。古印度人以为下雨是龙从天海中取水而洒下人间。中国人也接受这种说法，崐历本上注明几龙取水，表示今年雨量的多寡。龙王之中，有一位叫做沙竭罗龙王，崐他和幼女八岁时到释迦反牟尼所说法的灵鹫山前，转为男身，现佛之相。她成佛之崐时，为天龙八部所见。“夜叉”是佛经中的一种鬼神，有“夜叉八大将”、“十六大夜叉将”等名词。崐“夜叉”是本义是能吃鬼的神，又有敏捷、勇健、轻灵、秘密等意思。“维摩经”崐注：“什曰：‘夜叉有三种：一、在地，二、在空虚，三、天夜叉也。’”现在我崐们说到“夜叉”都是指恶鬼。但在佛经中，有很多夜叉是好的，夜叉八大将的任务崐是“维护众生界”。\r\n",
            "“乾达婆”是一种不吃酒内、只寻香气作为滋养的神，是服侍帝释的乐神之一，崐身上发出浓冽的香气，“乾达婆”在梵语中又是“变幻莫测”的意思，魔术师也叫崐“乾达婆”，海市蜃楼叫做“乾达婆城”。香气和音乐都是缥缈隐约，难以捉摸。\r\n",
            "“阿修罗”这种神道非常特别，男的极丑陋，而女的极美丽。阿修罗王常常率崐部和帝释战斗，因为阿修罗有美女而无美好食物，帝释有美食而无美女，互相妒忌崐抢夺，每有恶战，总是打得天翻地覆。我们常称惨遭轰炸、尸横遍地的大战场为“崐修罗场”，就是由此而来。大战的结果，阿修罗王往打败，，上崐天下地，无处可逃于是化身潜入藕的丝孔之中。阿修罗王性子暴躁、执拗而善妒。崐释迦牟尼说法，说“四念处”，阿修罗王也说法，说“五念处”；释迦牟尼说法“崐三十七道品”，阿修罗王偏又多一品，“说三十八道品”。佛经中的神话故事大都崐是譬喻。阿修罗王权力很大，能力很大，就是爱搞“老子不信邪”、“天下大乱，崐越乱越好”的事，阿修罗又疑心病很重，“大智度论卷三十五”：“阿修罗其心不崐端故，常疑于佛，谓佛助天。佛为说‘五众’，谓有六众，不为说一；若说‘四谛’崐，谓有五谛，不说一事。”“五众”即五蕴”，四谛是佛法中的基本观念。阿修罗崐听佛说法，疑心佛偏袒帝释，故意少说了一样。\r\n",
            "“迦楼罗”是一种大鸟，翅有种种庄严宝色，头上有一个大瘤，是如意珠，此崐鸟鸣声悲苦，以龙为食。旧说部中说岳飞是，“大鹏金翅鸟”投胎转世，迦楼罗就崐是大鹏金翅鸟，它每天要吃一个龙及五百条小龙。到它命终时，诸吐毒，无法再吃，崐于是上下翻飞七次，飞到金刚轮山顶上命终。因为它一生以龙(大毒蛇)为食物，体崐内积蓄毒气极多，临死时毒发自焚。肉身烧去后只余一心，作纯青琉璃色。\r\n",
            "“紧那罗”在梵语中为“人非人”之意。他形状和人一样，但头上生一只角，崐所以称为“人非人”，善于歌舞，是帝释的乐神。\r\n",
            "“摩呼罗迦”是大蟒神，人身而蛇头。这部小以“天龙八部”为名，写的是北崐宋时云南大理国的故事。\r\n",
            "大理国是佛教国家，皇帝都崇信佛教，往往放弃皇位，出家为僧，是我国历史崐上一个十分奇特的现象。据历史记载，大理国的皇帝中，圣德帝、孝德帝、宣仁帝、崐正廉帝、神宗等都避位为僧。“射雕英雄传”中所写的南帝段皇爷，就是大理国的崐皇帝。“天龙八部”的年代在“射雕英雄传”之前。本书故事发生于北宋哲宗无祜、崐绍圣年间，公元一○九四年前后。\r\n",
            "天龙八部这八种神道精怪，各有奇特个性和神通，虽是人间之外的众生，却也崐有尘世的欢喜和悲苦。这部小说里没有神道精怪，只是借用这个佛经名词，以象征崐一些现世人物，就象“水浒”中有母夜叉孙二娘、摩云金翅欧鹏。  \r\n",
            "\r\n",
            "正文 第一章 青衫磊落险峰行\r\n",
            "青光闪动，一柄青钢剑倏地刺出，指向在年汉子左肩，使剑少年不等招用老，腕抖剑斜，剑锋已削向那汉子右颈。那中年汉子剑挡格，铮的一声响，双剑相击，嗡嗡作声，震声未绝，双剑剑光霍霍，已拆了三招，中年汉子长剑猛地击落，直砍少年顶门。那少年避向右侧，左手剑诀一引，青钢剑疾刺那汉子大腿。\r\n",
            "两人剑法迅捷，全力相搏。\r\n",
            "练武厅东坐着二人。上首是个四十左右的中年道姑，铁青着脸，嘴唇紧闭。下首是个五十余岁的老者，右手捻着长须，神情甚是得意。两人的座位相距一丈有余，身后各站着二十余名男女弟子。西边一排椅子上坐着十余位宾客。东西双方的目光都集注于场中二人的角斗。\r\n",
            "眼见那少年与中年汉子已拆到七十余招，剑招越来越紧，兀自未分胜败。突然中年汉子一剑挥出，用力猛了，身子微微一幌，似欲摔跌。西边宾客中一个身穿青衫的年轻男子忍不住“嗤”的一声笑。他随即知道失态，忙伸手按住了口。\r\n",
            "便在这时，场中少年左手呼一掌拍出，击向那汉子后心，那汉子向前跨出一步避开，手中长剑蓦地圈转，喝一声：“着！”那少年左腿已然中剑，腿下一个踉跄，长剑在地下一撑，站直身子待欲再斗，那中年汉子已还剑入鞘，笑道：“褚师弟，承让、承让，伤得不厉害么？”那少年脸色苍白，咬着嘴唇道：“多谢龚师兄剑下留情。”\r\n",
            "那长须老者满脸得色，微微一笑，说道：“东宗已胜了三阵，看来这‘剑湖宫’又要让东宗再住五年了。辛师妹，咱们还须比下去么？”坐在他上首的那中年道姑强忍怒气，说道：“左师果然调教得好徒儿。但不知左师兄对‘无量玉壁’的钻研，这五年来可已大有心得么？”长须老者向她瞪了一眼，正色道：“师妹怎地忘了本派的规矩？”那道姑哼了一声，便不再说下去了。\r\n",
            "这老者姓左，名叫子穆，是“无量剑”东宗的掌门。那道姑姓辛，道号双清，是“无量剑”西宗掌门。\r\n",
            "“无量剑”原分东、北、西三宗，北宗近数十年来已趋式微，东西二宗却均人才鼎盛。“无量剑”于五代后唐年间在南诏无量山创派，掌门人居住无量山剑湖宫。自于大宋仁过年间分为三宗之后，每隔五年，三宗门下弟子便在剑湖宫中比武斗剑，获胜的一宗得在剑湖宫居住五年，至第六年上重行比试。五场斗剑，赢得三场者为胜。这五年之中，败者固然极力钻研，以图在下届剑会中洗雪前耻，胜者也是丝毫不敢松懈。北宗于四十年前获胜而入住剑湖宫，五年后败阵出宫，掌门人一怒而率领门人迁往山西，此后即不再参预比剑，与东西两宗也不通音问。三十五年来，东西二宗互有胜负。东宗胜过四次，西宗胜过两次。那龚姓中年汉子与褚姓少年相斗，已是本次比剑中的第四场，姓龚的汉子既胜，东宗四赛三胜，第五场便不用比了。\r\n",
            "西首锦凳上所坐的则是别派人士，其中有的是东西二宗掌门人共同出面邀请的公证人，其余则是前来观礼的嘉宾。这些人都是云南武林中的知名之士。只坐在最下首的那个青衣少年却是个无名之辈，偏是他在龚姓汉子伴作失足时嗤的一声笑。这少年乃随滇南普洱老武师马五德而来。马五德是大茶商，豪富好客，颇有孟尝之风，江湖上落魄的武师前去投奔，他必竭诚相待，因此人缘甚佳，武功却是平平。左子穆听马五德引见之时说这少年姓段，段姓是大理国的国姓，大理境内姓段的成千成万，左子穆当时听了也不以为意，心想分多半是马五德的弟子，这马老儿自身的功夫稀松平常，调教出来的弟子还高得到那里去，是以连“久仰”两字也懒得说，只拱了拱手，便肃入宾座。不料这年轻人不知天高地厚，竟当左子穆的得意弟子佯出虚招诱敌之时，失笑讥讽。\r\n",
            "当下左子穆笑道：“辛师妹今年派出的四名弟子，剑术上的造诣着实可观，尤其这第四场我们赢得更是侥幸。褚师侄年纪轻轻，居然练到了这般地步，前途当真不可限量，五年之后，只怕咱们东西宗得换换位了，呵呵，呵呵！”说着大笑不已，突然眼光一转，瞧向那姓段青年，说道：“我那劣徒适才以虚招‘跌扑步’获胜，这位段世兄似乎颇不以为然。便请段世兄下场指点小徒一二如何？马五哥威震滇南，强将手下无弱兵，段世兄的手段定是挺高的。”\r\n",
            "马五德脸上微微一红，忙道：“这位段兄弟不是我的弟子。你老哥哥这几手三脚猫的把式，怎配做人家师父？左贤弟可别当面取笑。这位段兄弟来到普洱舍下，听说我正要到无量山来，便跟着同来，说道无量山山水清幽，要来赏玩风景。”\r\n",
            "左子穆心想：“他若是你弟子，碍着你的面子，我也不能做得太绝了，既是寻常宾客，那可不能客气了。有人竟敢在剑湖宫中讥笑‘无量剑’东宗的武功，若不教他闹个灰头土脸下的山，姓左的颜面何存？”当下冷笑一声，说道：“请教段兄大号如何称呼，是那一位高人的门下？”\r\n",
            "那姓段青年微笑道：“在下单名一誉字，从来没学过什么武艺。我看到别人摔交，不论他真摔还是假摔，忍不住总是要笑的。”左子穆听他言语中全无恭敬之意，不禁心中有气，道：“那有什么好笑？”段誉轻摇手中摺扇，轻描淡写的道：“一个人站着坐着，没什么好笑，躺在床上，也不好笑，要是躺地下，哈哈，那就可笑得紧了。除非他是个三岁娃娃，那又作别论。”左子穆听他说话越来越狂妄，不禁气塞胸臆，向马五德道：“马五哥，这位段兄是你的好朋友么？”\r\n",
            "马五德和段誉也是初交，完全不知对方底细，他生性随和，段誉要同来无量山，他不便拒却，便带着来了，此时听左穆的口气甚是着恼，势必出手便极厉害，大好一个青年，何必让他吃个大亏？便道：“段兄弟和我虽无深交，咱们总是结伴来的。我瞧段兄弟斯斯文文的，未必会什么武功，适才这一笑定是出于无意。这样吧，老哥哥肚子也饿了，左贤弟赶快整治酒席，咱们贺你三杯。今日大好日子，左贤弟何必跟年轻晚辈计较？”\r\n",
            "左子穆道：“段兄既然不是马五哥的好朋友，那么兄弟如有得罪，也不算是扫了马五哥的金面。光杰，刚才人家笑你呢，你下场请教请教吧。”\r\n",
            "那中年汉子龚光杰巴不得师父有这句话，当下抽出长剑，往场中一站，倒转剑柄，拱手向段誉道：“段朋友，请！”段誉道：“很好，你练罢，我瞧着。”仍是坐在椅中，并不起身。龚光杰登时脸皮紫胀，怒道：“你……你说什么？”段誉道：“你手里拿了一把剑这么东晃来西去，想是要练剑，那么你就练罢。我向来不爱瞧人家动刀使剑，可是既来之，则安之，那也不防瞧着。”龚光杰喝道：“我师父叫你这小子也下场来，咱们比划比划。”\r\n",
            "段誉轻挥折扇，摇了摇头，说道：“你师父是你的师父，你师父可不是我的师父。你师父差得动你，你师父可差不动我。你师父叫你跟人家比剑，你已经跟人家比过了。你师父叫我跟你比剑，我一来不会，二来怕输，三来怕痛，四来怕死，因此是不比的。我说不比，就是不比。”\r\n",
            "他这番说什么“你师父”“我师父”的，说得犹如拗口令一般，练武厅中许多人听着，忍不住笑了出来。“无量剑”西宗双清门下男女各占其半，好几名女弟子格格娇笑。练武厅上庄严肃穆的气象，霎时间一扫无遗。\r\n",
            "龚光杰大踏步过来，伸剑指向段誉胸口，喝道：“你到底是真的不会，还是装傻？”段誉见剑尖离胸不过数寸，只须轻轻一送，便刺入了心脏，脸上却丝毫不露惊慌之色，说道：“我自然是真的不会，装傻有什么好装？”龚光杰道：“你到无量山剑湖宫中来撒野，想必是活得不耐烦了。你是何人门下？受谁的指使？若不直说，莫怪大爷剑下无情。”\r\n",
            "段誉道：：“你这位大爷怎地如此狠霸霸的？我平生最不爱瞧人打架。贵派叫做无量剑，住在无量山中。佛经有云：‘无量有四：一慈、二悲、三喜、四舍。’这‘四无量’么，众位当然明白：与乐之心为慈，拔苦之心为悲，喜众生离苦获乐之心曰喜，于一切众生舍怨亲之念而平等一如曰舍。无量寿佛者，阿弥陀佛也。阿弥陀佛，阿弥陀佛……”\r\n",
            "他唠叨叨的说佛念经，龚光杰长剑回收，突然左手挥出，拍的一声，结结实实的打了他一个耳光。段誉将头略侧，待欲闪避，对方手掌早已打过缩回，一张俊秀雪白的脸颊登时肿了起来，五个指印甚是清晰。\r\n",
            "这一来众人都是吃了一惊，眼见段誉漫不在乎，满嘴胡说八道的戏弄对方，料想必是身负绝艺，那知龚光杰随手一掌，他竟不能避开，看来当真是全然不会武功。武学高手故意装傻，玩弄敌手，那是常事，但决无不会武功之人如此胆大妄为的。龚光杰一掌得手，也不禁一呆，随即抓住段誉胸口，提起他身子，喝道：“我还道是什么了不起的人物，那知竟是脓包！”将他重重往地下摔落。段誉滚将出去，砰的一声，胸袋撞在桌脚上。\r\n",
            "马五德心中不忍，抢过去伸手扶起，说道：“原来老弟果然不会武功，那又何必到这里来厮混？”\r\n",
            "段誉摸了摸额角，说道：“我本是来游山玩水的，谁知道他们要比剑打架了？这样你砍我杀的，有什么好看？还不如瞧人家耍猴儿戏好玩得多。马五爷，再见，再见，我这可要走了。”\r\n",
            "左子穆身旁一名青弟子一跃而出，拦在段誉身前，说道：“你既不会武功，就这么夹着尾巴而走，那也罢了。怎么又说看我们比剑，还不如看耍猴儿戏？这话未免欺人太甚。我给你两条路走，要么跟我比划比划，叫你领教一下比耍猴儿也还不如的剑法；要么跟我师父磕八个响头，自己说三声‘放屁’！”段誉笑道：“你放屁？不怎么臭啊！”\r\n",
            "那人大怒，伸拳便向段誉面门击去，这一拳势夹劲风，眼见要打得他面青目肿，不料拳到中途，突然半空中飞下一件物事，缠住了那少年的手腕。这东西冷冰冰，滑腻腻，一缠上手腕，随即蠕蠕而动。那少年吃一惊，急忙缩手时，只见缠在腕上的竟是一条尺许长的赤练蛇，青红斑斓，甚是可怖。他大声惊呼，挥臂力振，但那蛇牢牢缠在腕上，说什么也甩不脱。忽然龚光杰大叫道：“蛇，蛇！”脸色大变，伸手插入自己衣领，到背心掏摸，但掏不到什么，只急得双足乱跳，手忙脚乱的解衣。\r\n",
            "这两下变故古怪之极，众人正惊奇间，忽听得头顶有人噗哧一笑。众人抬起头来，只见一个少女坐在梁上，双手抓的都是蛇。\r\n",
            "那少女约莫十六七岁年纪，一身青衫，笑靥如花，手中握着十来条尺许长小蛇。这些小蛇或青或花，头呈三角，均是毒蛇。但这少女拿在手上，便如是玩物一般毫不惧怕。众人向她仰视，也只是一瞥，听到龚光杰与他师弟大叫大嚷的惊呼，随即又都转眼去瞧那二人。\r\n",
            "段誉却仍是抬起了头望着她，见那少女双脚荡啊荡的，似乎这么坐梁上甚是好玩，问道：“姑娘，是你救我的么？”那少女道：“那恶人打你，你为什么不还手？”段誉摇头道：“我不会还手……”\r\n",
            "忽听得“啊”的一声，众人齐声叫唤，段誉低下头来，只见左穆手执长剑，剑锋上微带血痕，一条赤练蛇断成两截，掉在地下，显是被他挥剑斩死。龚光杰上身衣服已然脱光，赤了膊乱蹦乱跳，一条小青蛇在他背上游走，他反手欲捉，抓了几次都抓不到。\r\n",
            "左子穆喝道：“光杰，站着别动！”龚光杰一呆，只剑白光一闪，青蛇已断为两截，左子穆出剑如风，众人大都没瞧清楚他如何出手，青蛇已然斩断，而龚光杰背上丝毫无损。众人都高声喝起采来。\r\n",
            "梁上少女叫道：“喂，喂！长胡子老头，你干什么弄死了我两条蛇儿，我可要跟你不客气了。”\r\n",
            "左子穆怒道：“你是谁家女娃娃，到这儿来干什么？”心下暗暗纳罕，不知这少女何时爬到了梁上，竟然谁也没有知觉，虽说各人都凝神注视东西两宗比剑，但总不能不知头顶上伏着一个人，这件事传将出去，“无量剑”的人可丢得大了。但见那少女双脚一荡一荡，穿着一双葱绿色鞋儿绣着几朵小小黄花，纯然是小姑娘的打扮，左子穆又道：“快跳下来！”\r\n",
            "段誉忽道：“这么高，跳下来可不摔坏了么？你快叫人去拿架梯子来！”此言一出，又有人忍不住笑了起来。西宗门下几名女弟子均想：“此人一表人才，却原来是个大呆子。这少女既能神不知鬼不觉的上得梁去，轻功自然不弱，怎么要用梯子才爬得下来。”\r\n",
            "那少女道：“先赔了我的蛇儿，我再下来跟你说话。”左子穆道：“两条小蛇，有什么打紧，随便那里都可去捉两条来。”他见这少女玩毒物，若无其事，她本人年纪幼小，自不足畏，但她背后的师长父兄却只怕大有来头，因此言语中对她居然忍让三分。那少女笑道：“你倒说得容易，你去捉两条给我看看。”\r\n",
            "左子穆道：“快跳下来。”那少女道：“我不下来。”左子穆道：“你不下来，我可要上来拉了。“那少女格格一笑，道：“你试试看，拉得我下来，算你本事！”左子穆以一派宗师，终不能当着许多武林好手、门人弟子之前，跟一个小女孩闹着玩，便向双清道：“辛师妹，请你派一名女弟子上去抓她下来吧。”\r\n",
            "双清道：“西宗门下，没这么好的轻功，”左子穆脸色一沉，正要发话，那少女忽道：“你不赔我蛇儿，我给你个厉害瞧瞧！”从左腰皮囊里掏出一团毛茸茸的物事，向龚光杰掷了过去。\r\n",
            "龚光杰只道是件古怪暗器，不敢伸手去接，忙向旁边避开，不料这团毛茸茸的东西竟是活的，在半空中一扭，扑在龚光杰背上，众人这才看清，原来是只灰白色的小貂儿。这貂儿灵活已极，在龚光杰背上、胸前、脸上、颈中，迅捷无伦的奔来奔去。龚光杰双手急抓，可是他出手虽快，那貂儿更比他快了十倍，他每一下抓扑都落了空。旁人但见他双手急挥，在自己背上、胸前、脸上、颈中乱抓乱打，那貂儿却仍是游走不停。\r\n",
            "段誉笑道；“妙啊，妙啊，这貂儿有趣得紧。”\r\n",
            "这只小貂身长不满一尺，眼射红光，四脚爪子甚是锐利，片刻之间，龚光杰赤裸的上身已布满了一条条给貂爪抓出来的细血痕。\r\n",
            "忽听得那少女口中嘘嘘嘘的吹了几声。白影闪动，那貂儿扑到了龚光杰脸上，毛松松的尾巴向他眼上扫去。龚光杰双手急抓，貂儿早已奔到了他颈后，龚光杰的手指险些便插入了自己眼中。\r\n",
            "左子穆踏上两步，长剑倏地递出，这时那貂儿又已奔到龚光杰脸上，左子穆挺剑向貂儿刺去。貂儿身子一扭，早已奔到了龚光杰后颈，左子穆的剑尖及于徒儿眼皮而止。这一剑虽没刺到貂儿，旁观众人无不叹服，只须剑尖多递得半寸，龚光杰这只眼睛便是毁了。双清寻思：“左师兄剑术了得，非我所及，单是这招‘金针渡劫’，我怎能有这等造指？”\r\n",
            "刷刷刷刷，左子穆连出四剑，剑招虽然迅捷异常，那貂儿终究还是快一步。那少女叫道：“长胡子老头，你剑法很好。”口中尖声嘘嘘两下，那貂儿往下一窜，忽地不见了，左子穆一呆之际，只见龚光杰双手往大腿上乱抓乱摸，原来那貂儿已从裤脚管中钻入他裤中。\r\n",
            "段誉哈哈大笑，拍手说道：“今日当真是大开眼界，叹为观止了。”\r\n",
            "龚光杰手忙脚乱的除下长裤，露出两条生满黑毛的大腿。那少女叫道：“你这恶人爱欺侮人，叫你全身脱得清光，瞧你羞也不羞！”又是嘘嘘两声尖呼，那貂儿也真听话，爬上龚光杰左腿，立时钻入了他衬裤之中。练武厅上有不少女子，龚光杰这条衬裤是无论如何不肯脱的，双足乱跳，双手在自己小腹、屁股上拍了一阵，大叫一声，跌跌撞撞的往外直奔。\r\n",
            "他刚奔到厅门，忽然门外抢进一个人来，砰的一声，两人撞了个满怀。这一出一入，势道都是奇急，龚光杰踉跄后退，门外进来那人却仰天一交，摔倒在地。\r\n",
            "左子穆失声叫道：“容师弟！”\r\n",
            "龚光杰也顾不得裤中那只貂儿兀自从左腿爬到右腿，又从右腿爬上屁股，忙抢上将那人扶起，貂儿突然爬到了他前阴的要紧所在。他“啊”一声大叫，双手忙去抓貂，那人又即摔倒。\r\n",
            "梁上少女格格娇笑，说道：“整得你也够了！”“嘶”的一声长呼叫。貂儿从龚光杰裤中钻了出来，沿墙直上，奔到梁上，白影一闪，回到那少女怀中。那少女赞道：“乖貂儿！”右手指两手指抓着一条小蛇的尾巴，倒提起来，在貂儿面前晃动。那貂儿前脚抓住，张口便吃，原来那少女手中这许多小蛇都是喂貂的食料。\r\n",
            "段誉前所未见，看得津津有味，见貂儿吃完一条小蛇，钻入了那少女腰间的皮囊。\r\n",
            "龚光杰再次扶起那人，惊叫：“容师叔，你……你怎么啦！”左穆抢上前去只见师弟容子矩双目圆睁，满脸愤恨之色，口鼻中却没了气息。左子穆大惊，忙施推拿，已然无法救活。左子穆知道容子矩武功虽较已为逊，比龚光杰高得多了，这么一撞，他居然没能避开，而一撞之下登时毙命，那定是进来之前已然身受重伤，忙解开他上衣查察伤势。衣衫解开，只见他胸口赫然写着八个黑字：“神农帮诛灭无量剑”。众人不约而同的大声惊呼。\r\n",
            "这八个黑字深入肌理，既非墨笔书写，也不是用尖利之物刻划而致，竟是以剧毒的药物写就，腐蚀之下，深陷肌肤。\r\n",
            "左穆略一凝视，不禁大怒，手中长剑一振，嗡嗡作响，喝道：“且瞧是神农帮诛灭无量剑，还是无量剑诛灭神农帮。此仇不报，何以为人？”再看容子矩身子各处，并无其他伤痕，喝道：“光豪、光杰，外面瞧瞧去！”\r\n",
            "干光豪、龚光杰两名大弟子各挺长剑，应声而出。\r\n",
            "这一来厅上登时大乱，各人再不也去理会段誉和那梁上少女，围住了容子矩的尸身纷纷议论。马五德沉吟道：“神农帮闹得越来越不成话了。左贤弟，不知他们如何跟贵派结下了梁子。”\r\n",
            "左子穆心伤师弟惨亡，哽咽道：“是为了采药。去年秋天，神农帮四名香主来剑湖宫求见，要到我们后山采几味药。采药本来没什么大不了，神农帮原是以采药、贩药为生，跟我们无量剑虽没什么交情，却也没有梁子。但马五哥想必知道，我们这后山轻易不能让外人进入，别说神农帮跟我们只是泛泛之交，便是各位好朋友，也从来没去后山游玩过。这只是祖师爷传下的规矩，我们做小辈的不敢违犯而已，其实也没什么要紧……”\r\n",
            "梁上那少女将手中十条蛇放入腰间的一个小竹篓里，从怀里摸出一把瓜子来吃，两只脚仍是一荡一荡的，忽然将一粒瓜子往段誉头上掷去，正中他额头，笑道：“喂，你吃不吃瓜？上来吧！”\r\n",
            "段誉道：“没梯子，我上不来。”那少女道：“这个容易！”从腰间解下一条绿色绸带，垂了下来，道：“你抓住带子，我拉你上来。”段誉道：“我身子重，你拉不动的。”那少女笑道：“试试看嘛，摔你不死的。”段誉见衣带挂到面前，伸手便握住了。那少女道：“抓紧了！”轻轻一提段誉身子已然离地。那少女双手互拉扯，几下但将他拉上横梁。\r\n",
            "段誉道：“你这只小貂儿真好玩，这么听话。”那少女从皮囊中摸出小貂，双手捧着。段誉见貂儿皮毛润滑，一双红眼精光闪闪瞧着自己，甚是可爱，问道：“我摸摸它不打紧吗？”那少女道：“你摸好了。”段誉伸手在貂背上轻轻抚摸，只觉着手轻软温暖。\r\n",
            "突然之间，那貂儿嗤的一声，钻入了少女腰间的皮囊。段誉没提防，向后一缩，一个没坐稳，险些摔跌下去。那少女抓住他后领，拉他靠近自己身边，笑道：“你当直一点儿也不会武功，那可就奇了。”段誉道：“有什么奇怪？”那少女道：“你不会武功，却单身到这儿来，那是定会给这些恶人欺侮的。你来干什么？”\r\n",
            "段誉正要相告，忽得脚步声响，干光豪、龚光杰两人奔进大厅。\r\n",
            "这时龚光杰已穿回了长裤，上身却仍是光着膀子。两人神色间颇有惊惶之意，走到左子穆跟前。干光豪道：“师父，神农帮在对面山上聚集，把守了山道，说道谁也不许下山。咱们见敌方人多，不得师父号令，没敢随便动手。”左子穆道：“嗯，来了多少人？”干光豪道：“大约七八十人。”左子穆嘿嘿冷笑，道：“七八十人，便想诛灭无量剑了？只怕也没没这么容易。”\r\n",
            "龚光杰道：“他们用箭射过来一封信封，皮上写得好生无礼。”说着将信呈上。\r\n",
            "左子穆见们封上写着：“字谕左子穆”五个大字，便不接信，说道：“你拆来瞧瞧。”龚光杰道：“是！”拆开信封，抽出信笺。\r\n",
            "那少女在段誉耳边低声道：“打你的这个恶人便要死了。”段誉道：“为什么？”那少女低声道：“信封信笺上都是毒。”段誉道：“那有这么厉害？”\r\n",
            "只听龚光杰读道：“神农帮字谕左……听者(他不敢直呼师父之名，读到“左”字时，便将下面“子穆”二字略过不念)：限尔等一个进辰之内，自断右手，折断兵刃，退出无量山剑湖宫，否则无量剑鸡犬不留。”\r\n",
            "无量剑西宗掌门双清冷笑道：“神农帮是什么东西，夸下好大的海口！”\r\n",
            "突然间砰的一声，龚光杰仰天便倒。干光豪站在他身旁，忙叫：“师弟！”伸手欲扶。左子穆抢上两步，翻掌按在他的胸口，轻力微吐，将他震出三步，喝道：“只怕有毒，别碰他身子！”只见龚光杰脸上肌肉不住抽搐，拿信的一只手掌霎时之间便成深黑，双足挺了几下，便已死去。\r\n",
            "前后只过一顿饭功夫，“无量剑”东宗连死了两名好手，众人无不骇然。\r\n",
            "段誉低声道：“你也是神农帮的么？”那少女嗔道：“呸！我才不是呢，你胡说八道什么？”段誉道：“那你怎地知道信上有毒？”那少女笑道：“这下毒的功夫粗浅得紧，一眼便瞧出来了。这些笨法儿只能害害无知之徒。”她这几句话厅上众人都听见了，一齐抬起头来，只见她兀自咬着瓜子，穿着花鞋的一双脚不住前后晃荡。\r\n",
            "左子穆向龚光杰手中拿着的那信瞧去，不见有何异状，侧过了头再看，果见信封和信笺上隐隐有磷光闪动，心中一凛，抬头向那少女道：“姑娘尊姓大名？”那少女道：“我的尊姓大名，可不能跟你说，这叫做天机不可泄漏。”在这当口还听到两句话，左子穆怒火直冒，强自忍耐，才不发作，说道：“那么令尊是谁？尊师是那一位？”那少女笑道：“哈哈，我才不上你的当呢。我跟你说我令尊是谁，你便知道我的尊姓了。你既知我尊姓，便查得到我的大名了，我的尊师便是我妈。我妈的名字更加不能跟你说。”\r\n",
            "左子穆听她语声既娇且糯，是云南本地人无疑，寻思：“云南武林中，有那一擅于轻功的夫妇会是她的父母？”那少女没出过手，无法从她武功家数上推想，便道：“姑娘请下来，一起商议对策。神农帮说谁也不许下山，连你也要杀了。”\r\n",
            "那少女笑道：“他们不会杀我的，神农帮只杀无量剑的人。我在路上听到了消息，因此赶来瞧瞧杀人的热闹。长胡子老头，你们剑法不错，可是不会使毒，斗不过神农帮的。”\r\n",
            "这几句正说中了“无量剑”的弱点，若凭真实的功夫厮拼，无量剑东西宗，再加上八位聘请前来作公证的各派好手，无论如何不会敌不过神农帮，但说到用毒，各人却一窍不通。\r\n",
            "左穆听她口吻中全是幸灾乐祸之意，似乎“无量剑”越死得人多，她越加看得开心，当下冷哼一声，问道：“姑娘在路上听到什么消息？”他一向颐指气使惯了，随便一句话，似乎都叫人非好好回答不可。\r\n",
            "那少女忽问：“你吃瓜子不吃？”\r\n",
            "左子穆脸色微微发紫，若不是大敌在外，早已发作，当强忍怒气，道：“不吃！”\r\n",
            "段誉插口道：“你这是什么瓜子？桂花？玫瑰？还是松子味的？”那少女道：“啊哟！瓜子还有许多讲究么？我可不知道了。我这瓜子是妈妈用蛇胆炒的，常吃眼目明亮，你试试看。”说着抓了一把，塞在段誉手中，又道：“吃不惯的人，觉得有点儿苦，其实很好吃的。”段誉不便拂她之意，拿了一粒瓜子送入口中，入口果觉辛涩，但略加辨味，便似谏果回甘，舌底生津，当下接连吃了起来。他将吃过的瓜子壳一片片的放在梁上，那少女却肆无忌惮，顺口便往下吐出。瓜子壳在众人头顶上乱飞，许多人都皱眉避开。\r\n",
            "左子穆又问：“姑娘在道上听到什么消息，若能见告，在下……在下感激不尽。”他为了探听消息，言语只得十分客气。那少女道：“我听神农帮的说什么‘无量玉壁’，那是什么玩意儿？”左子穆一怔，说道：“无量玉壁？难道无量山中有什么宝玉、宝壁么？倒没听见过。双清师妹，你听人说过么？”双清还未回答，那少女抢着道：“他自然没听说过。你俩不用一搭一挡做戏，不肯说，那就干脆别说。哼，好稀罕么？”\r\n",
            "左子穆神色尴尬，说道：“啊，我想起来了，神农帮所说的，多半是无量山白龙峰畔的镜面石。这块石头平滑如镜，能照见毛发，有人说是块美玉，其实呢，只是一块又白又光的石头罢了。”\r\n",
            "那少女道：“你早些说了，岂不是好？你怎么跟神农帮结的怨家啊？干么他们要将你无量剑杀得鸡犬不留？”\r\n",
            "左子穆眼见反客为主之势已成，要想这少女透露什么消息，非得自己先说不可，目下事势紧迫，又当着这许多外客，总不能抓下这小姑娘来强加拷问，便道：“姑娘请下来，待我详加奉告。”那少女双脚荡了荡，说道：“详加奉告，那倒不用，反正你的话有真有假，我也只信得了这么三成四成，你随便说一些吧。”\r\n",
            "左子穆双眉一竖，脸现怒容，随即收敛，说道：“去年神农帮要到我们后山采药，我没答允。他们便来偷采。我师弟容子矩和几名弟子撞见了，出言责备。他们说道：‘这里又不是金銮殿、御花园，外人为什么来不得？难道无量山你们无量剑买下的么？，双方言语冲突，动起手来。容师弟下手没留情，杀了他们二人。梁子便是这样结下的。后来在澜沧江畔，双方又动一次手，再欠下了几条人命。”那少女道：“嗯，原来如此。他们要采的什么药？”左子穆道：“这个倒不大清楚。”\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AhQ8-7YFndK",
        "outputId": "66cf9a7d-fdd4-4230-a979-9e6f8695c81f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-15 04:21:25.075237: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-15 04:21:25.092849: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765772485.114476    4151 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765772485.120902    4151 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765772485.137993    4151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765772485.138019    4151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765772485.138022    4151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765772485.138025    4151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-15 04:21:25.142990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[32m2025-12-15 04:21:33.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mModel args: ModelArguments(model_name_or_path='Qwen/Qwen2.5-0.5B', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True)\u001b[0m\n",
            "\u001b[32m2025-12-15 04:21:33.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/pretrain', validation_file_dir='./data/pretrain', max_train_samples=20000, max_eval_samples=10, streaming=False, block_size=128, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1, keep_linebreaks=True)\u001b[0m\n",
            "\u001b[32m2025-12-15 04:21:33.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m361\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=30000,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=50,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=True,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs-pt-v1/runs/Dec15_04-21-33_099816dd262b,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs-pt-v1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=3,\n",
            "per_device_train_batch_size=3,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=outputs-pt-v1,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=50,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.05,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-12-15 04:21:33.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False)\u001b[0m\n",
            "\u001b[32m2025-12-15 04:21:33.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m363\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
            "tokenizer_config.json: 7.23kB [00:00, 27.6MB/s]\n",
            "vocab.json: 2.78MB [00:00, 65.3MB/s]\n",
            "merges.txt: 1.67MB [00:00, 131MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 179MB/s]\n",
            "\u001b[32m2025-12-15 04:21:34.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m471\u001b[0m - \u001b[1mtrain files: ['./data/pretrain/tianlongbabu.txt', './data/pretrain/fever.txt', './data/pretrain/en_article_tail500.txt']\u001b[0m\n",
            "\u001b[32m2025-12-15 04:21:34.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m481\u001b[0m - \u001b[1meval files: ['./data/pretrain/tianlongbabu.txt', './data/pretrain/fever.txt', './data/pretrain/en_article_tail500.txt']\u001b[0m\n",
            "Generating train split: 3876 examples [00:00, 220432.57 examples/s]\n",
            "Generating validation split: 3876 examples [00:00, 387286.43 examples/s]\n",
            "\u001b[32m2025-12-15 04:21:35.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m513\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "})\u001b[0m\n",
            "Running tokenizer on dataset: 100% 3876/3876 [00:09<00:00, 411.24 examples/s]\n",
            "Running tokenizer on dataset: 100% 3876/3876 [00:09<00:00, 409.88 examples/s]\n",
            "Grouping texts in chunks of 128: 100% 3876/3876 [00:00<00:00, 9100.35 examples/s]\n",
            "Grouping texts in chunks of 128: 100% 3876/3876 [00:00<00:00, 9194.78 examples/s]\n",
            "\u001b[32m2025-12-15 04:22:00.514\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m576\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 2502\u001b[0m\n",
            "\u001b[32m2025-12-15 04:22:00.514\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m577\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
            "\u001b[32m2025-12-15 04:22:00.516\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m578\u001b[0m - \u001b[34m\u001b[1m天龙八部\n",
            "\n",
            "\n",
            "正文 释名\n",
            "“天龙八部”这名词出于佛经。许多大乘佛经叙述佛向诸菩萨、比丘等说法时，崐常有天龙八部参与听法。如“法华经：提婆达多品”：“天龙八部、人与非人，皆崐遥见彼龙女成佛”。\n",
            "“非人”，包括八种神道怪物，因为以“天”及“龙”为首，崐所以称为《天龙八部》。八部罗，七归那\u001b[0m\n",
            "\u001b[32m2025-12-15 04:22:00.517\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m590\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2025-12-15 04:22:00.517\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m591\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
            "\u001b[32m2025-12-15 04:22:00.519\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m592\u001b[0m - \u001b[34m\u001b[1m天龙八部\n",
            "\n",
            "\n",
            "正文 释名\n",
            "“天龙八部”这名词出于佛经。许多大乘佛经叙述佛向诸菩萨、比丘等说法时，崐常有天龙八部参与听法。如“法华经：提婆达多品”：“天龙八部、人与非人，皆崐遥见彼龙女成佛”。\n",
            "“非人”，包括八种神道怪物，因为以“天”及“龙”为首，崐所以称为《天龙八部》。八部罗，七归那\u001b[0m\n",
            "config.json: 100% 681/681 [00:00<00:00, 4.49MB/s]\n",
            "model.safetensors: 100% 988M/988M [00:01<00:00, 538MB/s]\n",
            "generation_config.json: 100% 138/138 [00:00<00:00, 821kB/s]\n",
            "\u001b[32m2025-12-15 04:22:03.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m651\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2025-12-15 04:22:03.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m656\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
            "\u001b[32m2025-12-15 04:22:03.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m669\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "\u001b[32m2025-12-15 04:22:03.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m670\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
            "trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
            "  warnings.warn(\n",
            "/content/MedicalGPT/pretraining.py:700: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SavePeftModelTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = SavePeftModelTrainer(\n",
            "\u001b[32m2025-12-15 04:22:04.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m715\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
            "\u001b[32m2025-12-15 04:22:04.879\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m716\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[ 64272,   3837,  99364,  91676, 100292,  99296,   8997,  99212,  57750,\n",
            "         104462,  49567,  34187, 111502,   3837,  88970,  42411, 102080,  19403,\n",
            "          70074,   3837, 104094,  36987, 107314,   3837,  41406,  56568, 113240,\n",
            "         104543,  75758,    198,  37474,  99783,  44793,  36987,  35946,  49187,\n",
            "         105665,  49828, 108145,   3837, 114811,  63109, 104543,   3837, 104984,\n",
            "          97706,  99756,  34187,  56568, 106669,  81264,    198,  99212,  57750,\n",
            "         104462, 104494,  99592, 109760,   3837, 101611,  99313,  99234,  22243,\n",
            "           3837,  56652, 106718, 109482,  63703, 110861,   3837,  99508, 100524,\n",
            "          99793,   1773, 100147,  36407,  37474,  99783,  30440,  80158,  99746,\n",
            "          34187,   3837,  64355,  99684,  44934,  99336,  89012,  44793, 101913,\n",
            "          82647,  99385, 101432,  49828, 112656, 101724, 119463,   1773,  99212,\n",
            "          57750, 104462,  99882,  44793,  36987,  56568,  79072,  16530, 113206,\n",
            "          81264,  37474,  99783, 108585, 104543,  44793,  36987,  56568,  43288,\n",
            "         109593,  52801, 119026,   9370, 103357, 100897, 104340,  75758,  99212,\n",
            "          57750, 104462],\n",
            "        [  3837, 115041, 100090,  99805,   3837,  69041,  63703, 105967, 108739,\n",
            "         111897,  64355,   1773,  63703, 105967, 105626, 101760,   1773, 113251,\n",
            "         100432,  44793,  36987,  99783,  99261,   3837,  56568,  99665, 103335,\n",
            "         110460,   3837, 104219,  99245,  74763,  62922,  99172,   3837, 105418,\n",
            "          33126, 100186,  32555,  99369,  17177, 109913,   3837, 102786,  99418,\n",
            "         100406,  99309, 108681,   3837, 104201,  20412,  50511,  18830,  53930,\n",
            "          46423,   3837, 105622,  99851, 102633,  32945,  37474,  99783, 106101,\n",
            "          34187,   3837,  99539,  77144,  99901,  22382,   8997,  21894,  99237,\n",
            "          33108, 100288, 107899,  71618, 108928, 113916,   3837,  48934,  14777,\n",
            "         100898,  99180,   3837,  99364,  59879,  18493,  37474,  99783,  33447,\n",
            "          99931,   9370,  99208,  99468, 102811,  17447,   3837,  14777,  82075,\n",
            "          63367,  47534, 116345,  99844,  17254,   1773,  99212,  99208,  99468,\n",
            "         102811,  99372,  28291,  99326,  14777, 101490,   3837, 100409,  99625,\n",
            "         100297,   1773, 104578,  21894,  48921, 109503,  27442,  42411,  48888,\n",
            "         100297, 101168],\n",
            "        [ 56006, 102346,   1773,  56568, 107575, 111285, 107558,  56006, 103954,\n",
            "           3837,  35946,  14777,  36407,  99670,   3837,  40820,  36407,  99756,\n",
            "          25710,   3837,  44991,  36407,  99756, 100406,   3837,  63703,  36407,\n",
            "          99756,  99561,   3837, 101886,  20412,  16530,  56006,   9370,   1773,\n",
            "         101435,  16530,  56006,   3837,  99486,  16530,  56006,  32945,    198,\n",
            "          42411,  43288,  86117, 107585,   2073,  56568, 107575,    854,   2073,\n",
            "          35946, 107575,  97907,   3837, 109622, 108228, 119326,  39426,  99738,\n",
            "         100141,   3837,  99930,  99669,  99928,  15946, 108264, 110311,   3837,\n",
            "         106118, 103206,  99898,  53647,  42192,  32757, 103954,    854,  60686,\n",
            "         100401,  99493,  79766,  64689,  16872, 105569,  99200,  99571,  41146,\n",
            "          99369,   3837,  52801, 116446,  57750, 105770,  33983,  33983, 105219,\n",
            "          48738,   1773,  99930,  99669,  99928,  17447,  99939, 106625, 105696,\n",
            "           9370, 105632,   3837, 119501,  20450,  14777,  99864,  42192,  99712,\n",
            "           8997, 111315,  99225, 102028,  26288, 100875,  64682, 101180,   3837,\n",
            "         100867, 103954]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'labels': tensor([[ 64272,   3837,  99364,  91676, 100292,  99296,   8997,  99212,  57750,\n",
            "         104462,  49567,  34187, 111502,   3837,  88970,  42411, 102080,  19403,\n",
            "          70074,   3837, 104094,  36987, 107314,   3837,  41406,  56568, 113240,\n",
            "         104543,  75758,    198,  37474,  99783,  44793,  36987,  35946,  49187,\n",
            "         105665,  49828, 108145,   3837, 114811,  63109, 104543,   3837, 104984,\n",
            "          97706,  99756,  34187,  56568, 106669,  81264,    198,  99212,  57750,\n",
            "         104462, 104494,  99592, 109760,   3837, 101611,  99313,  99234,  22243,\n",
            "           3837,  56652, 106718, 109482,  63703, 110861,   3837,  99508, 100524,\n",
            "          99793,   1773, 100147,  36407,  37474,  99783,  30440,  80158,  99746,\n",
            "          34187,   3837,  64355,  99684,  44934,  99336,  89012,  44793, 101913,\n",
            "          82647,  99385, 101432,  49828, 112656, 101724, 119463,   1773,  99212,\n",
            "          57750, 104462,  99882,  44793,  36987,  56568,  79072,  16530, 113206,\n",
            "          81264,  37474,  99783, 108585, 104543,  44793,  36987,  56568,  43288,\n",
            "         109593,  52801, 119026,   9370, 103357, 100897, 104340,  75758,  99212,\n",
            "          57750, 104462],\n",
            "        [  3837, 115041, 100090,  99805,   3837,  69041,  63703, 105967, 108739,\n",
            "         111897,  64355,   1773,  63703, 105967, 105626, 101760,   1773, 113251,\n",
            "         100432,  44793,  36987,  99783,  99261,   3837,  56568,  99665, 103335,\n",
            "         110460,   3837, 104219,  99245,  74763,  62922,  99172,   3837, 105418,\n",
            "          33126, 100186,  32555,  99369,  17177, 109913,   3837, 102786,  99418,\n",
            "         100406,  99309, 108681,   3837, 104201,  20412,  50511,  18830,  53930,\n",
            "          46423,   3837, 105622,  99851, 102633,  32945,  37474,  99783, 106101,\n",
            "          34187,   3837,  99539,  77144,  99901,  22382,   8997,  21894,  99237,\n",
            "          33108, 100288, 107899,  71618, 108928, 113916,   3837,  48934,  14777,\n",
            "         100898,  99180,   3837,  99364,  59879,  18493,  37474,  99783,  33447,\n",
            "          99931,   9370,  99208,  99468, 102811,  17447,   3837,  14777,  82075,\n",
            "          63367,  47534, 116345,  99844,  17254,   1773,  99212,  99208,  99468,\n",
            "         102811,  99372,  28291,  99326,  14777, 101490,   3837, 100409,  99625,\n",
            "         100297,   1773, 104578,  21894,  48921, 109503,  27442,  42411,  48888,\n",
            "         100297, 101168],\n",
            "        [ 56006, 102346,   1773,  56568, 107575, 111285, 107558,  56006, 103954,\n",
            "           3837,  35946,  14777,  36407,  99670,   3837,  40820,  36407,  99756,\n",
            "          25710,   3837,  44991,  36407,  99756, 100406,   3837,  63703,  36407,\n",
            "          99756,  99561,   3837, 101886,  20412,  16530,  56006,   9370,   1773,\n",
            "         101435,  16530,  56006,   3837,  99486,  16530,  56006,  32945,    198,\n",
            "          42411,  43288,  86117, 107585,   2073,  56568, 107575,    854,   2073,\n",
            "          35946, 107575,  97907,   3837, 109622, 108228, 119326,  39426,  99738,\n",
            "         100141,   3837,  99930,  99669,  99928,  15946, 108264, 110311,   3837,\n",
            "         106118, 103206,  99898,  53647,  42192,  32757, 103954,    854,  60686,\n",
            "         100401,  99493,  79766,  64689,  16872, 105569,  99200,  99571,  41146,\n",
            "          99369,   3837,  52801, 116446,  57750, 105770,  33983,  33983, 105219,\n",
            "          48738,   1773,  99930,  99669,  99928,  17447,  99939, 106625, 105696,\n",
            "           9370, 105632,   3837, 119501,  20450,  14777,  99864,  42192,  99712,\n",
            "           8997, 111315,  99225, 102028,  26288, 100875,  64682, 101180,   3837,\n",
            "         100867, 103954]], device='cuda:0')}\u001b[0m\n",
            "{'loss': 3.5633, 'grad_norm': 2.5613150596618652, 'learning_rate': 4.7619047619047615e-06, 'epoch': 0.0}\n",
            "{'loss': 3.8672, 'grad_norm': 2.2047383785247803, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.01}\n",
            "{'loss': 3.8904, 'grad_norm': 2.129809856414795, 'learning_rate': 9.523809523809524e-05, 'epoch': 0.02}\n",
            "{'loss': 3.8674, 'grad_norm': 2.685133218765259, 'learning_rate': 0.00014285714285714287, 'epoch': 0.04}\n",
            "{'loss': 3.6632, 'grad_norm': 3.9101529121398926, 'learning_rate': 0.00019047619047619048, 'epoch': 0.05}\n",
            "{'loss': 3.5855, 'grad_norm': 2.627802848815918, 'learning_rate': 0.000197979797979798, 'epoch': 0.06}\n",
            "  6% 50/834 [00:16<03:50,  3.39it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 4.239707946777344, 'eval_accuracy': 0.31496062992125984, 'eval_runtime': 0.319, 'eval_samples_per_second': 31.347, 'eval_steps_per_second': 12.539, 'epoch': 0.06}\n",
            "  6% 50/834 [00:16<03:50,  3.39it/s]\n",
            "100% 4/4 [00:00<00:00, 22.28it/s]\u001b[A\n",
            "{'loss': 3.6738, 'grad_norm': 2.6379568576812744, 'learning_rate': 0.00019545454545454548, 'epoch': 0.07}\n",
            "{'loss': 3.5147, 'grad_norm': 2.731538772583008, 'learning_rate': 0.00019292929292929293, 'epoch': 0.08}\n",
            "{'loss': 3.5237, 'grad_norm': 2.9196348190307617, 'learning_rate': 0.0001904040404040404, 'epoch': 0.1}\n",
            "{'loss': 3.4853, 'grad_norm': 2.610560655593872, 'learning_rate': 0.0001878787878787879, 'epoch': 0.11}\n",
            "{'loss': 3.6234, 'grad_norm': 2.5121195316314697, 'learning_rate': 0.00018535353535353537, 'epoch': 0.12}\n",
            " 12% 100/834 [00:31<03:31,  3.47it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.154722690582275, 'eval_accuracy': 0.3165354330708661, 'eval_runtime': 0.2807, 'eval_samples_per_second': 35.624, 'eval_steps_per_second': 14.249, 'epoch': 0.12}\n",
            " 12% 100/834 [00:31<03:31,  3.47it/s]\n",
            "100% 4/4 [00:00<00:00, 22.10it/s]\u001b[A\n",
            "{'loss': 3.4973, 'grad_norm': 2.923976421356201, 'learning_rate': 0.00018282828282828283, 'epoch': 0.13}\n",
            "{'loss': 3.5678, 'grad_norm': 2.5145530700683594, 'learning_rate': 0.0001803030303030303, 'epoch': 0.14}\n",
            "{'loss': 3.7498, 'grad_norm': 2.5957212448120117, 'learning_rate': 0.00017777777777777779, 'epoch': 0.16}\n",
            "{'loss': 3.6158, 'grad_norm': 2.6598830223083496, 'learning_rate': 0.00017525252525252527, 'epoch': 0.17}\n",
            "{'loss': 3.4216, 'grad_norm': 2.6665048599243164, 'learning_rate': 0.00017272727272727275, 'epoch': 0.18}\n",
            " 18% 150/834 [00:47<03:19,  3.43it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.274066925048828, 'eval_accuracy': 0.3102362204724409, 'eval_runtime': 0.283, 'eval_samples_per_second': 35.333, 'eval_steps_per_second': 14.133, 'epoch': 0.18}\n",
            " 18% 150/834 [00:47<03:19,  3.43it/s]\n",
            "100% 4/4 [00:00<00:00, 21.64it/s]\u001b[A\n",
            "{'loss': 3.5619, 'grad_norm': 2.694812774658203, 'learning_rate': 0.0001702020202020202, 'epoch': 0.19}\n",
            "{'loss': 3.6209, 'grad_norm': 2.9824681282043457, 'learning_rate': 0.00016767676767676768, 'epoch': 0.2}\n",
            "{'loss': 3.5998, 'grad_norm': 2.6585257053375244, 'learning_rate': 0.00016515151515151516, 'epoch': 0.22}\n",
            "{'loss': 3.5892, 'grad_norm': 2.489856719970703, 'learning_rate': 0.00016262626262626264, 'epoch': 0.23}\n",
            "{'loss': 3.4279, 'grad_norm': 2.424548864364624, 'learning_rate': 0.00016010101010101012, 'epoch': 0.24}\n",
            " 24% 200/834 [01:02<03:11,  3.31it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.268102645874023, 'eval_accuracy': 0.3102362204724409, 'eval_runtime': 0.2785, 'eval_samples_per_second': 35.903, 'eval_steps_per_second': 14.361, 'epoch': 0.24}\n",
            " 24% 200/834 [01:02<03:11,  3.31it/s]\n",
            "100% 4/4 [00:00<00:00, 22.32it/s]\u001b[A\n",
            "{'loss': 3.6132, 'grad_norm': 2.6975858211517334, 'learning_rate': 0.00015757575757575757, 'epoch': 0.25}\n",
            "{'loss': 3.4974, 'grad_norm': 2.516862630844116, 'learning_rate': 0.00015505050505050508, 'epoch': 0.26}\n",
            "{'loss': 3.2998, 'grad_norm': 2.7125463485717773, 'learning_rate': 0.00015252525252525253, 'epoch': 0.28}\n",
            "{'loss': 3.4931, 'grad_norm': 5.358119487762451, 'learning_rate': 0.00015000000000000001, 'epoch': 0.29}\n",
            "{'loss': 3.4299, 'grad_norm': 2.5451862812042236, 'learning_rate': 0.00014747474747474747, 'epoch': 0.3}\n",
            " 30% 250/834 [01:18<02:53,  3.37it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.200314521789551, 'eval_accuracy': 0.31811023622047246, 'eval_runtime': 0.2807, 'eval_samples_per_second': 35.624, 'eval_steps_per_second': 14.25, 'epoch': 0.3}\n",
            " 30% 250/834 [01:18<02:53,  3.37it/s]\n",
            "100% 4/4 [00:00<00:00, 22.25it/s]\u001b[A\n",
            "{'loss': 3.3616, 'grad_norm': 2.5367591381073, 'learning_rate': 0.00014494949494949495, 'epoch': 0.31}\n",
            "{'loss': 3.4404, 'grad_norm': 2.542215585708618, 'learning_rate': 0.00014242424242424243, 'epoch': 0.32}\n",
            "{'loss': 3.3935, 'grad_norm': 2.374197244644165, 'learning_rate': 0.0001398989898989899, 'epoch': 0.34}\n",
            "{'loss': 3.3451, 'grad_norm': 2.2895925045013428, 'learning_rate': 0.0001373737373737374, 'epoch': 0.35}\n",
            "{'loss': 3.4765, 'grad_norm': 2.297074317932129, 'learning_rate': 0.00013484848484848484, 'epoch': 0.36}\n",
            " 36% 300/834 [01:33<02:42,  3.28it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.12955379486084, 'eval_accuracy': 0.32125984251968503, 'eval_runtime': 0.2844, 'eval_samples_per_second': 35.163, 'eval_steps_per_second': 14.065, 'epoch': 0.36}\n",
            " 36% 300/834 [01:34<02:42,  3.28it/s]\n",
            "100% 4/4 [00:00<00:00, 22.09it/s]\u001b[A\n",
            "{'loss': 3.3287, 'grad_norm': 2.242051124572754, 'learning_rate': 0.00013232323232323235, 'epoch': 0.37}\n",
            "{'loss': 3.4586, 'grad_norm': 2.6499862670898438, 'learning_rate': 0.0001297979797979798, 'epoch': 0.38}\n",
            "{'loss': 3.3906, 'grad_norm': 2.8918333053588867, 'learning_rate': 0.00012727272727272728, 'epoch': 0.4}\n",
            "{'loss': 3.4608, 'grad_norm': 2.520266532897949, 'learning_rate': 0.00012474747474747473, 'epoch': 0.41}\n",
            "{'loss': 3.6014, 'grad_norm': 2.52290415763855, 'learning_rate': 0.00012222222222222224, 'epoch': 0.42}\n",
            " 42% 350/834 [01:49<02:22,  3.39it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.154479026794434, 'eval_accuracy': 0.3188976377952756, 'eval_runtime': 0.2764, 'eval_samples_per_second': 36.177, 'eval_steps_per_second': 14.471, 'epoch': 0.42}\n",
            " 42% 350/834 [01:49<02:22,  3.39it/s]\n",
            "100% 4/4 [00:00<00:00, 22.44it/s]\u001b[A\n",
            "{'loss': 3.4073, 'grad_norm': 2.6492300033569336, 'learning_rate': 0.00011969696969696971, 'epoch': 0.43}\n",
            "{'loss': 3.7189, 'grad_norm': 2.9954895973205566, 'learning_rate': 0.00011717171717171717, 'epoch': 0.44}\n",
            "{'loss': 3.4544, 'grad_norm': 2.3128867149353027, 'learning_rate': 0.00011464646464646464, 'epoch': 0.46}\n",
            "{'loss': 3.43, 'grad_norm': 2.500103235244751, 'learning_rate': 0.00011212121212121212, 'epoch': 0.47}\n",
            "{'loss': 3.3427, 'grad_norm': 2.6296374797821045, 'learning_rate': 0.0001095959595959596, 'epoch': 0.48}\n",
            " 48% 400/834 [02:04<02:08,  3.38it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.155652046203613, 'eval_accuracy': 0.3236220472440945, 'eval_runtime': 0.2848, 'eval_samples_per_second': 35.112, 'eval_steps_per_second': 14.045, 'epoch': 0.48}\n",
            " 48% 400/834 [02:05<02:08,  3.38it/s]\n",
            "100% 4/4 [00:00<00:00, 21.57it/s]\u001b[A\n",
            "{'loss': 3.2988, 'grad_norm': 2.3732776641845703, 'learning_rate': 0.00010707070707070708, 'epoch': 0.49}\n",
            "{'loss': 3.5626, 'grad_norm': 3.0647408962249756, 'learning_rate': 0.00010454545454545455, 'epoch': 0.5}\n",
            "{'loss': 3.5666, 'grad_norm': 2.2627382278442383, 'learning_rate': 0.00010202020202020202, 'epoch': 0.52}\n",
            "{'loss': 3.6134, 'grad_norm': 2.36572527885437, 'learning_rate': 9.94949494949495e-05, 'epoch': 0.53}\n",
            "{'loss': 3.2696, 'grad_norm': 2.4284732341766357, 'learning_rate': 9.696969696969698e-05, 'epoch': 0.54}\n",
            " 54% 450/834 [02:20<01:55,  3.33it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.141730308532715, 'eval_accuracy': 0.3188976377952756, 'eval_runtime': 0.2875, 'eval_samples_per_second': 34.782, 'eval_steps_per_second': 13.913, 'epoch': 0.54}\n",
            " 54% 450/834 [02:20<01:55,  3.33it/s]\n",
            "100% 4/4 [00:00<00:00, 21.64it/s]\u001b[A\n",
            "{'loss': 3.3147, 'grad_norm': 2.281677484512329, 'learning_rate': 9.444444444444444e-05, 'epoch': 0.55}\n",
            "{'loss': 3.5409, 'grad_norm': 2.886303186416626, 'learning_rate': 9.191919191919192e-05, 'epoch': 0.56}\n",
            "{'loss': 3.2219, 'grad_norm': 2.237278699874878, 'learning_rate': 8.93939393939394e-05, 'epoch': 0.58}\n",
            "{'loss': 3.4644, 'grad_norm': 2.359229564666748, 'learning_rate': 8.686868686868688e-05, 'epoch': 0.59}\n",
            "{'loss': 3.443, 'grad_norm': 2.3919296264648438, 'learning_rate': 8.434343434343435e-05, 'epoch': 0.6}\n",
            " 60% 500/834 [02:36<01:39,  3.34it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.081844329833984, 'eval_accuracy': 0.32677165354330706, 'eval_runtime': 0.2847, 'eval_samples_per_second': 35.129, 'eval_steps_per_second': 14.052, 'epoch': 0.6}\n",
            " 60% 500/834 [02:36<01:39,  3.34it/s]\n",
            "100% 4/4 [00:00<00:00, 21.83it/s]\u001b[A\n",
            "{'loss': 3.4276, 'grad_norm': 2.4654018878936768, 'learning_rate': 8.181818181818183e-05, 'epoch': 0.61}\n",
            "{'loss': 3.3549, 'grad_norm': 2.6803596019744873, 'learning_rate': 7.92929292929293e-05, 'epoch': 0.62}\n",
            "{'loss': 3.2188, 'grad_norm': 2.6480281352996826, 'learning_rate': 7.676767676767676e-05, 'epoch': 0.64}\n",
            "{'loss': 3.0345, 'grad_norm': 2.2631990909576416, 'learning_rate': 7.424242424242424e-05, 'epoch': 0.65}\n",
            "{'loss': 3.419, 'grad_norm': 2.8195483684539795, 'learning_rate': 7.171717171717171e-05, 'epoch': 0.66}\n",
            " 66% 550/834 [02:51<01:25,  3.31it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.052846908569336, 'eval_accuracy': 0.3362204724409449, 'eval_runtime': 0.2845, 'eval_samples_per_second': 35.145, 'eval_steps_per_second': 14.058, 'epoch': 0.66}\n",
            " 66% 550/834 [02:52<01:25,  3.31it/s]\n",
            "100% 4/4 [00:00<00:00, 21.86it/s]\u001b[A\n",
            "{'loss': 3.4749, 'grad_norm': 2.780660629272461, 'learning_rate': 6.91919191919192e-05, 'epoch': 0.67}\n",
            "{'loss': 3.3965, 'grad_norm': 2.8510425090789795, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.68}\n",
            "{'loss': 3.3707, 'grad_norm': 2.8969855308532715, 'learning_rate': 6.414141414141415e-05, 'epoch': 0.7}\n",
            "{'loss': 3.4218, 'grad_norm': 2.651984930038452, 'learning_rate': 6.161616161616162e-05, 'epoch': 0.71}\n",
            "{'loss': 3.4739, 'grad_norm': 2.486553192138672, 'learning_rate': 5.90909090909091e-05, 'epoch': 0.72}\n",
            " 72% 600/834 [03:07<01:10,  3.34it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.057465553283691, 'eval_accuracy': 0.3291338582677165, 'eval_runtime': 0.2835, 'eval_samples_per_second': 35.269, 'eval_steps_per_second': 14.108, 'epoch': 0.72}\n",
            " 72% 600/834 [03:07<01:10,  3.34it/s]\n",
            "100% 4/4 [00:00<00:00, 21.75it/s]\u001b[A\n",
            "{'loss': 3.3989, 'grad_norm': 2.4030497074127197, 'learning_rate': 5.6565656565656563e-05, 'epoch': 0.73}\n",
            "{'loss': 3.4687, 'grad_norm': 2.1985676288604736, 'learning_rate': 5.4040404040404044e-05, 'epoch': 0.74}\n",
            "{'loss': 3.31, 'grad_norm': 2.5688083171844482, 'learning_rate': 5.151515151515152e-05, 'epoch': 0.76}\n",
            "{'loss': 3.5492, 'grad_norm': 2.4451370239257812, 'learning_rate': 4.898989898989899e-05, 'epoch': 0.77}\n",
            "{'loss': 3.3209, 'grad_norm': 2.508202314376831, 'learning_rate': 4.6464646464646464e-05, 'epoch': 0.78}\n",
            " 78% 650/834 [03:23<00:54,  3.36it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.019860744476318, 'eval_accuracy': 0.331496062992126, 'eval_runtime': 0.2733, 'eval_samples_per_second': 36.586, 'eval_steps_per_second': 14.634, 'epoch': 0.78}\n",
            " 78% 650/834 [03:23<00:54,  3.36it/s]\n",
            "100% 4/4 [00:00<00:00, 22.75it/s]\u001b[A\n",
            "{'loss': 3.1848, 'grad_norm': 2.365760326385498, 'learning_rate': 4.3939393939393944e-05, 'epoch': 0.79}\n",
            "{'loss': 3.5006, 'grad_norm': 2.6014811992645264, 'learning_rate': 4.141414141414142e-05, 'epoch': 0.8}\n",
            "{'loss': 3.2768, 'grad_norm': 2.570929765701294, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.82}\n",
            "{'loss': 3.417, 'grad_norm': 2.541093111038208, 'learning_rate': 3.6363636363636364e-05, 'epoch': 0.83}\n",
            "{'loss': 3.4797, 'grad_norm': 2.5186753273010254, 'learning_rate': 3.3838383838383844e-05, 'epoch': 0.84}\n",
            " 84% 700/834 [03:38<00:39,  3.39it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.037746906280518, 'eval_accuracy': 0.33700787401574805, 'eval_runtime': 0.2787, 'eval_samples_per_second': 35.88, 'eval_steps_per_second': 14.352, 'epoch': 0.84}\n",
            " 84% 700/834 [03:39<00:39,  3.39it/s]\n",
            "100% 4/4 [00:00<00:00, 22.26it/s]\u001b[A\n",
            "{'loss': 3.3762, 'grad_norm': 2.7855937480926514, 'learning_rate': 3.131313131313132e-05, 'epoch': 0.85}\n",
            "{'loss': 3.3894, 'grad_norm': 2.7633180618286133, 'learning_rate': 2.878787878787879e-05, 'epoch': 0.86}\n",
            "{'loss': 3.2918, 'grad_norm': 2.5136125087738037, 'learning_rate': 2.6262626262626268e-05, 'epoch': 0.88}\n",
            "{'loss': 3.3864, 'grad_norm': 2.550044059753418, 'learning_rate': 2.3737373737373738e-05, 'epoch': 0.89}\n",
            "{'loss': 3.397, 'grad_norm': 2.361889123916626, 'learning_rate': 2.1212121212121215e-05, 'epoch': 0.9}\n",
            " 90% 750/834 [03:54<00:24,  3.38it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.015854358673096, 'eval_accuracy': 0.3354330708661417, 'eval_runtime': 0.2848, 'eval_samples_per_second': 35.115, 'eval_steps_per_second': 14.046, 'epoch': 0.9}\n",
            " 90% 750/834 [03:54<00:24,  3.38it/s]\n",
            "100% 4/4 [00:00<00:00, 21.77it/s]\u001b[A\n",
            "{'loss': 3.4106, 'grad_norm': 2.732651710510254, 'learning_rate': 1.8686868686868688e-05, 'epoch': 0.91}\n",
            "{'loss': 3.4041, 'grad_norm': 2.9258787631988525, 'learning_rate': 1.6161616161616165e-05, 'epoch': 0.92}\n",
            "{'loss': 3.3771, 'grad_norm': 2.9032704830169678, 'learning_rate': 1.3636363636363637e-05, 'epoch': 0.94}\n",
            "{'loss': 3.2285, 'grad_norm': 2.5032448768615723, 'learning_rate': 1.1111111111111112e-05, 'epoch': 0.95}\n",
            "{'loss': 3.3541, 'grad_norm': 2.4238548278808594, 'learning_rate': 8.585858585858587e-06, 'epoch': 0.96}\n",
            " 96% 800/834 [04:10<00:09,  3.40it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.009690284729004, 'eval_accuracy': 0.3346456692913386, 'eval_runtime': 0.2835, 'eval_samples_per_second': 35.273, 'eval_steps_per_second': 14.109, 'epoch': 0.96}\n",
            " 96% 800/834 [04:10<00:09,  3.40it/s]\n",
            "100% 4/4 [00:00<00:00, 22.15it/s]\u001b[A\n",
            "{'loss': 3.4064, 'grad_norm': 2.5987801551818848, 'learning_rate': 6.060606060606061e-06, 'epoch': 0.97}\n",
            "{'loss': 3.2964, 'grad_norm': 2.440812349319458, 'learning_rate': 3.5353535353535352e-06, 'epoch': 0.98}\n",
            "{'loss': 3.4981, 'grad_norm': 3.888909101486206, 'learning_rate': 1.0101010101010103e-06, 'epoch': 1.0}\n",
            "{'train_runtime': 261.3689, 'train_samples_per_second': 9.573, 'train_steps_per_second': 3.191, 'train_loss': 3.4558639457757523, 'epoch': 1.0}\n",
            "100% 834/834 [04:21<00:00,  3.19it/s]\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               =   648356GF\n",
            "  train_loss               =     3.4559\n",
            "  train_runtime            = 0:04:21.36\n",
            "  train_samples            =       2502\n",
            "  train_samples_per_second =      9.573\n",
            "  train_steps_per_second   =      3.191\n",
            "\u001b[32m2025-12-15 04:26:27.161\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m733\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 261.3689, 'train_samples_per_second': 9.573, 'train_steps_per_second': 3.191, 'total_flos': 696167143243776.0, 'train_loss': 3.4558639457757523, 'epoch': 1.0, 'train_samples': 2502}\u001b[0m\n",
            "\u001b[32m2025-12-15 04:26:27.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m734\u001b[0m - \u001b[1mSaving model checkpoint to outputs-pt-v1\u001b[0m\n",
            "\u001b[32m2025-12-15 04:26:27.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m742\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "100% 4/4 [00:00<00:00, 19.53it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_accuracy           =     0.3362\n",
            "  eval_loss               =      4.007\n",
            "  eval_runtime            = 0:00:00.28\n",
            "  eval_samples            =         10\n",
            "  eval_samples_per_second =      35.36\n",
            "  eval_steps_per_second   =     14.144\n",
            "  perplexity              =    54.9809\n",
            "\u001b[32m2025-12-15 04:26:28.204\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m755\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 4.006986141204834, 'eval_accuracy': 0.3362204724409449, 'eval_runtime': 0.2828, 'eval_samples_per_second': 35.36, 'eval_steps_per_second': 14.144, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 54.980915890184676}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python pretraining.py \\\n",
        "    --model_name_or_path Qwen/Qwen2.5-0.5B \\\n",
        "    --train_file_dir ./data/pretrain \\\n",
        "    --validation_file_dir ./data/pretrain \\\n",
        "    --per_device_train_batch_size 3 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --seed 42 \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 20000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --eval_strategy steps \\\n",
        "    --save_steps 50 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --block_size 128 \\\n",
        "    --group_by_length True \\\n",
        "    --output_dir outputs-pt-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjQ5dV8wFndK",
        "outputId": "7cb64101-c031-4326-9bc4-597ce6a86546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 22M\n",
            "-rw-r--r-- 1 root root  721 Dec 15 04:26 adapter_config.json\n",
            "-rw-r--r-- 1 root root  17M Dec 15 04:26 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root  605 Dec 15 04:26 added_tokens.json\n",
            "-rw-r--r-- 1 root root  471 Dec 15 04:26 all_results.json\n",
            "drwxr-xr-x 2 root root 4.0K Dec 15 04:26 \u001b[0m\u001b[01;34mcheckpoint-750\u001b[0m/\n",
            "drwxr-xr-x 2 root root 4.0K Dec 15 04:26 \u001b[01;34mcheckpoint-800\u001b[0m/\n",
            "drwxr-xr-x 2 root root 4.0K Dec 15 04:26 \u001b[01;34mcheckpoint-834\u001b[0m/\n",
            "-rw-r--r-- 1 root root  262 Dec 15 04:26 eval_results.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 15 04:26 merges.txt\n",
            "-rw-r--r-- 1 root root 5.0K Dec 15 04:26 README.md\n",
            "drwxr-xr-x 3 root root 4.0K Dec 15 04:22 \u001b[01;34mruns\u001b[0m/\n",
            "-rw-r--r-- 1 root root  616 Dec 15 04:26 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 7.1K Dec 15 04:26 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  20K Dec 15 04:26 trainer_state.json\n",
            "-rw-r--r-- 1 root root  229 Dec 15 04:26 train_results.json\n",
            "-rw-r--r-- 1 root root 3.3M Dec 15 04:26 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh outputs-pt-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op0t63cxFndK"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Y5Fl5L04FndK"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU7ZF0EWFndK",
        "outputId": "697791aa-1906-46dd-83bc-35b149486a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-15 04:27:47.224596: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-15 04:27:47.242053: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765772867.264125    5921 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765772867.271462    5921 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765772867.288581    5921 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765772867.288617    5921 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765772867.288620    5921 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765772867.288623    5921 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-15 04:27:47.293805: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Namespace(base_model='Qwen/Qwen2.5-0.5B', tokenizer_path=None, lora_model='outputs-pt-v1', resize_emb=False, output_dir='merged-pt/', hf_hub_model_id='', hf_hub_token=None)\n",
            "Base model: Qwen/Qwen2.5-0.5B\n",
            "LoRA model: outputs-pt-v1\n",
            "Loading LoRA for causal language model\n",
            "Merging with merge_and_unload...\n",
            "Saving to Hugging Face format...\n",
            "Done! model saved to merged-pt/\n"
          ]
        }
      ],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model Qwen/Qwen2.5-0.5B --lora_model outputs-pt-v1 --output_dir merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYpogOwCFndK",
        "outputId": "86823645-60c8-4c92-d787-d923ccb5aa91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 958M\n",
            "-rw-r--r-- 1 root root  605 Dec 15 04:27 added_tokens.json\n",
            "-rw-r--r-- 1 root root  744 Dec 15 04:27 config.json\n",
            "-rw-r--r-- 1 root root  117 Dec 15 04:27 generation_config.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 15 04:27 merges.txt\n",
            "-rw-r--r-- 1 root root 943M Dec 15 04:27 model.safetensors\n",
            "-rw-r--r-- 1 root root  616 Dec 15 04:27 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 7.1K Dec 15 04:27 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Dec 15 04:27 tokenizer.json\n",
            "-rw-r--r-- 1 root root 2.7M Dec 15 04:27 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbJVfKxoFndK"
      },
      "outputs": [],
      "source": [
        "%cat merged-pt/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of63jAMIFndK"
      },
      "source": [
        "Stage1 增量预训练完成。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "36Fk5LeqFndL"
      },
      "source": [
        "# Stage 2: Supervised FineTuning\n",
        "\n",
        "第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图，并注入领域知识\n",
        "\n",
        "| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "JpnD1wCKFndL"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B 或者 Stage1得到的预训练模型\n",
        "2. 数据集：SFT阶段使用的是使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "D9qYGNVrFndL"
      },
      "source": [
        "## Stage2 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T13:58:38.966506Z",
          "start_time": "2023-06-15T13:58:38.778132Z"
        },
        "id": "edL5LHFQFndL"
      },
      "outputs": [],
      "source": [
        "%ls ./data/finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icyD_G0MFndL",
        "outputId": "48b22282-fd1d-45fc-e2bc-5cf6b1eb45d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-15 04:28:44.724712: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-15 04:28:44.742228: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765772924.763460    6207 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765772924.769960    6207 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765772924.786776    6207 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765772924.786809    6207 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765772924.786812    6207 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765772924.786815    6207 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-15 04:28:44.791806: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[32m2025-12-15 04:28:51.633\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m192\u001b[0m - \u001b[33m\u001b[1mYou may set max_train_samples = -1 to run all samples in production.\u001b[0m\n",
            "\u001b[32m2025-12-15 04:28:51.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m346\u001b[0m - \u001b[1mModel args: ModelArguments(model_name_or_path='merged-pt', load_in_8bit=False, load_in_4bit=False, tokenizer_name_or_path=None, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True, rope_scaling=None, flash_attn=False, shift_attn=False, neft_alpha=0)\u001b[0m\n",
            "\u001b[32m2025-12-15 04:28:51.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m347\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/finetune', validation_file_dir='./data/finetune', max_train_samples=1000, max_eval_samples=10, ignore_pad_token_for_loss=True, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1)\u001b[0m\n",
            "\u001b[32m2025-12-15 04:28:51.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m348\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=30000,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=50,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs-sft-v1/runs/Dec15_04-28-51_099816dd262b,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs-sft-v1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=outputs-sft-v1,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.05,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.05,\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-12-15 04:28:51.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m349\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, train_on_inputs=False, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False, model_max_length=512, template_name='vicuna')\u001b[0m\n",
            "\u001b[32m2025-12-15 04:28:51.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
            "\u001b[32m2025-12-15 04:28:52.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m376\u001b[0m - \u001b[1mAdd bos_token: <|endoftext|>, bos_token_id: 151643\u001b[0m\n",
            "\u001b[32m2025-12-15 04:28:52.051\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m383\u001b[0m - \u001b[34m\u001b[1mTokenizer: Qwen2Tokenizer(name_or_path='merged-pt', vocab_size=151643, model_max_length=131072, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\u001b[0m\n",
            "\u001b[32m2025-12-15 04:28:52.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m411\u001b[0m - \u001b[1mtrain files: ['./data/finetune/numina_cot_sharegpt_data_1k.jsonl', './data/finetune/sharegpt_zh_1K_format.jsonl', './data/finetune/medical_sft_1K_format.jsonl']\u001b[0m\n",
            "\u001b[32m2025-12-15 04:28:52.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m416\u001b[0m - \u001b[1meval files: ['./data/finetune/numina_cot_sharegpt_data_1k.jsonl', './data/finetune/sharegpt_zh_1K_format.jsonl', './data/finetune/medical_sft_1K_format.jsonl']\u001b[0m\n",
            "Generating train split: 3000 examples [00:00, 110727.06 examples/s]\n",
            "Generating validation split: 3000 examples [00:00, 171270.65 examples/s]\n",
            "\u001b[32m2025-12-15 04:28:52.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m432\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'conversations'],\n",
            "        num_rows: 3000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'conversations'],\n",
            "        num_rows: 3000\n",
            "    })\n",
            "})\u001b[0m\n",
            "\u001b[32m2025-12-15 04:28:52.443\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m534\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'id': None, 'conversations': [{'from': 'human', 'value': '盆腔炎吃什么药好呢？，肚子痛，往下坠的痛，白带特多，不能够大力的行走，总想大便。在乎怎样的帮助：治疗盆腔炎的药物'}, {'from': 'gpt', 'value': '盆腔炎治疗主要抱括药物治疗，头孢菌素或者克林霉素以及氧氟沙星等，还可用些活血化瘀，清热解毒的中药，要是有脓肿的话，药物治疗要是不好还可考虑手术解决。您的总想大便，应留意是不是有脓肿构成的情况，平时留意性生活卫生，及时治疗传染的情况，预防工作再次发生后遗症'}]}\u001b[0m\n",
            "Running tokenizer on dataset: 100% 1000/1000 [00:09<00:00, 103.22 examples/s]\n",
            "Filter: 100% 995/995 [00:00<00:00, 2383.35 examples/s]\n",
            "\u001b[32m2025-12-15 04:29:05.260\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m551\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 995\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:05.260\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:05.262\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m553\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]:\n",
            "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 盆腔炎吃什么药好呢？，肚子痛，往下坠的痛，白带特多，不能够大力的行走，总想大便。在乎怎样的帮助：治疗盆腔炎的药物 ASSISTANT:盆腔炎治疗主要抱括药物治疗，头孢菌素或者克林霉素以及氧氟沙星等，还可用些活血化瘀，清热解毒的中药，要是有脓肿的话，药物治疗要是不好还可考虑手术解决。您的总想大便，应留意是不是有脓肿构成的情况，平时留意性生活卫生，及时治疗传染的情况，预防工作再次发生后遗症<|endoftext|>\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:05.264\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m556\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]:\n",
            "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>盆腔炎治疗主要抱括药物治疗，头孢菌素或者克林霉素以及氧氟沙星等，还可用些活血化瘀，清热解毒的中药，要是有脓肿的话，药物治疗要是不好还可考虑手术解决。您的总想大便，应留意是不是有脓肿构成的情况，平时留意性生活卫生，及时治疗传染的情况，预防工作再次发生后遗症<|endoftext|>\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:05.265\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m570\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:05.265\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m574\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'id': 'AI-MO/NuminaMath-CoT-0', 'conversations': [{'from': 'human', 'value': \"Consider the terms of an arithmetic sequence: $-\\\\frac{1}{3}, y+2, 4y, \\\\ldots$. Solve for $y$. Let's think step by step and output the final answer within \\\\boxed{}.\"}, {'from': 'gpt', 'value': 'For an arithmetic sequence, the difference between consecutive terms must be equal. Therefore, we can set up the following equations based on the sequence given:\\n\\\\[ (y + 2) - \\\\left(-\\\\frac{1}{3}\\\\right) = 4y - (y+2) \\\\]\\n\\nSimplify and solve these equations:\\n\\\\[ y + 2 + \\\\frac{1}{3} = 4y - y - 2 \\\\]\\n\\\\[ y + \\\\frac{7}{3} = 3y - 2 \\\\]\\n\\\\[ \\\\frac{7}{3} + 2 = 3y - y \\\\]\\n\\\\[ \\\\frac{13}{3} = 2y \\\\]\\n\\\\[ y = \\\\frac{13}{6} \\\\]\\n\\nThus, the value of $y$ that satisfies the given arithmetic sequence is $\\\\boxed{\\\\frac{13}{6}}$.'}]}\u001b[0m\n",
            "Running tokenizer on validation dataset: 100% 10/10 [00:00<00:00, 321.14 examples/s]\n",
            "Filter: 100% 10/10 [00:00<00:00, 1305.38 examples/s]\n",
            "\u001b[32m2025-12-15 04:29:08.055\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m584\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:08.055\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m585\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:08.057\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m586\u001b[0m - \u001b[34m\u001b[1mA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: Consider the terms of an arithmetic sequence: $-\\frac{1}{3}, y+2, 4y, \\ldots$. Solve for $y$. Let's think step by step and output the final answer within \\boxed{}. ASSISTANT:For an arithmetic sequence, the difference between consecutive terms must be equal. Therefore, we can set up the following equations based on the sequence given:\n",
            "\\[ (y + 2) - \\left(-\\frac{1}{3}\\right) = 4y - (y+2) \\]\n",
            "\n",
            "Simplify and solve these equations:\n",
            "\\[ y + 2 + \\frac{1}{3} = 4y - y - 2 \\]\n",
            "\\[ y + \\frac{7}{3} = 3y - 2 \\]\n",
            "\\[ \\frac{7}{3} + 2 = 3y - y \\]\n",
            "\\[ \\frac{13}{3} = 2y \\]\n",
            "\\[ y = \\frac{13}{6} \\]\n",
            "\n",
            "Thus, the value of $y$ that satisfies the given arithmetic sequence is $\\boxed{\\frac{13}{6}}$.<|endoftext|>\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:08.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m705\u001b[0m - \u001b[1m🔧 大模型训练配置:\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:08.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m706\u001b[0m - \u001b[1m  model_kwargs: {'config': Qwen2Config {\n",
            "  \"_name_or_path\": \"merged-pt\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            ", 'torch_dtype': torch.bfloat16, 'trust_remote_code': True, 'quantization_config': None, 'low_cpu_mem_usage': True, 'device_map': 'auto'}\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:08.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m713\u001b[0m - \u001b[1m✅ 模型加载完成\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:08.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m716\u001b[0m - \u001b[1m📊 模型分布情况:\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:08.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m718\u001b[0m - \u001b[1m🔧 使用HuggingFace设备映射:\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:08.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m720\u001b[0m - \u001b[1m  : 0\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:08.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m728\u001b[0m - \u001b[1m📈 设备使用统计:\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:08.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m730\u001b[0m - \u001b[1m  0: 1 个模块\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:08.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m751\u001b[0m - \u001b[1m💾 GPU内存使用情况:\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:08.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m756\u001b[0m - \u001b[1m  GPU 0: 已分配=0.9GB, 缓存=1.0GB, 总计=79.3GB\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:08.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m798\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:08.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m813\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:08.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m822\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:08.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m823\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
            "trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "\u001b[32m2025-12-15 04:29:09.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m845\u001b[0m - \u001b[1mGradient checkpointing enabled.\u001b[0m\n",
            "/content/MedicalGPT/supervised_finetuning.py:862: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SavePeftModelTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = SavePeftModelTrainer(\n",
            "\u001b[32m2025-12-15 04:29:09.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m874\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:09.213\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m876\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[    32,   6236,   1948,  ..., 151643, 151643, 151643],\n",
            "        [    32,   6236,   1948,  ..., 151643, 151643, 151643],\n",
            "        [    32,   6236,   1948,  ..., 151643, 151643, 151643],\n",
            "        [    32,   6236,   1948,  ...,  47822, 151643, 151643]],\n",
            "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 0]], device='cuda:0'), 'labels': tensor([[  -100,   -100,   -100,  ...,   -100,   -100,   -100],\n",
            "        [  -100,   -100,   -100,  ...,   -100,   -100,   -100],\n",
            "        [  -100,   -100,   -100,  ...,   -100,   -100,   -100],\n",
            "        [  -100,   -100,   -100,  ...,  47822, 151643,   -100]],\n",
            "       device='cuda:0')}\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:09.268\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m877\u001b[0m - \u001b[34m\u001b[1minput_ids:\n",
            "[tensor([    32,   6236,   1948,    264,  22208,   1196,    323,    458,  20443,\n",
            "         11229,  17847,     13,    576,  17847,   6696,  10950,     11,  11682,\n",
            "            11,    323,  47787,  11253,    311,    279,   1196,    594,   4755,\n",
            "          3918,     82,     29,   6448,     25,  42849,    122, 103639,  31935,\n",
            "         57218,  99789,  31914, 112437, 100179,  26232, 100018,  11622, 101037,\n",
            "         11319,   3837,  42192,  35560,   3846,   2821,     25, 100345, 103929,\n",
            "         53481, 100141,  85106, 107878,  37029,   3837,  85106,  88991, 105372,\n",
            "        100649, 102100, 104361, 105373, 118406,   3837, 104406,  17447, 100634,\n",
            "        118287,   3837,  86744, 105251,  86744, 104460, 104579,   1773, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643],\n",
            "       device='cuda:0'), tensor([    32,   6236,   1948,    264,  22208,   1196,    323,    458,  20443,\n",
            "         11229,  17847,     13,    576,  17847,   6696,  10950,     11,  11682,\n",
            "            11,    323,  47787,  11253,    311,    279,   1196,    594,   4755,\n",
            "          3918,     82,     29,   6448,     25,  21658,    678,    279,  19703,\n",
            "           315,    198,  78045,   1147,     61,     21,    481,   1147,     61,\n",
            "            19,    488,   1147,     61,     17,    481,    220,     16,    284,\n",
            "           220,     15,     11,   1124,    921,   1782,   7192,  49952,    949,\n",
            "           315,    264,   3704,    646,    387,  13302,    438,  57960,   9407,\n",
            "          1124,  16827,  54876,   1380,    400,     12,     24,     15,  24884,\n",
            "         43298,   1124,    273,   1124,  16827,   1124,    273,    220,     24,\n",
            "            15,  24884,  43298,  12947,   7379,  57960,  16827,  12947,   6771,\n",
            "           594,   1744,   3019,    553,   3019,    323,   2550,    279,   1590,\n",
            "          4226,   2878,   1124,  79075,  46391,  35560,   3846,   2821,     25,\n",
            "          5338,     11,    582,  18130,    279,  23606,   2097,   5503,    304,\n",
            "           264,   4428,  11153,    510,  78045,    320,     89,     61,     17,\n",
            "           481,    220,     16,   2376,     89,     61,     21,    481,   1147,\n",
            "            61,     19,    488,   1147,     61,     17,    481,    220,     16,\n",
            "             8,    284,   1147,     61,     23,    481,    220,     16,    284,\n",
            "           220,     15,     13,   1124,    921,   1986,  23945,    198,  78045,\n",
            "          1147,     61,     23,    284,    220,     16,   1124,  26243,   1147,\n",
            "           284,   1124,   7884,    606,     90,  78055,     92,   1124,   2359,\n",
            "         11520,  37018,     90,     18,     21,     15,  24884,  43298,   1124,\n",
            "         50853,    595,  15170,     23,  11035,   1291,      8,    284,   1124,\n",
            "          7884,    606,     90,  78055,     92,    320,     19,     20,  24884,\n",
            "         43298,   1124,  50853,    595,      8,   1124,    921,   1958,   1045,\n",
            "          7546,    400,     74,  54876,    448,    400,     74,    284,    220,\n",
            "            15,     11,    220,     16,     11,    220,     17,     11,  60353,\n",
            "           220,     22,      3,    382,  11209,     11,    311,   1477,    279,\n",
            "         19703,    315,    279,   4024,  10807,  23606,     11,    582,   1083,\n",
            "          1184,    311,  21687,  19703,    315,    400,     89,     61,     17,\n",
            "           284,    220,     16,  54876,    600,   1734,   2572,    400,     89,\n",
            "           284,   1124,   5187,     16,  12947,   1096,  41208,  11508,    311,\n",
            "          1447,     12,  57960,   7884,    606,     90,  78055,     92,    220,\n",
            "            19,     20,  24884,  43298,  25046,     12,  57960,   7884,    606,\n",
            "            90,  78055,     92,    220,     16,     18,     20,  24884,  43298,\n",
            "         25046,     12,  57960,   7884,    606,     90,  78055,     92,    220,\n",
            "            17,     17,     20,  24884,  43298,  25046,     12,  57960,   7884,\n",
            "           606,     90,  78055,     92,    220,     18,     16,     20,  24884,\n",
            "         43298,  66426,  12549,    582,    525,   8014,    304,    279,   7192,\n",
            "         49952,    949,     11,  15251,    438,  57960,   9407,   1124,  16827,\n",
            "         54876,    582,  15442,    279,  75259,   2750,    518,   1493,   3501,\n",
            "           510,     12,  57960,   9407,      7,     19,     20,  24884,  43298,\n",
            "             8,    284,   1124,  37018,  35702,  26888,     90,     17,   3417,\n",
            "            90,     17,     92,  25046,     12,  57960,   9407,      7,     16,\n",
            "            18,     20,  24884,  43298,      8,    284,    481,     59,  37018,\n",
            "         35702,  26888,     90,     17,   3417,     90,     17,     92,  25046,\n",
            "            12,  57960,   9407,      7,     17,     17,     20,  24884,  43298,\n",
            "             8,    284,    481,     59,  37018,  35702,  26888,     90,     17,\n",
            "          3417,     90,     17,     92,  25046,     12,  57960,   9407,      7,\n",
            "            18,     16,     20,  24884,  43298,      8,    284,   1124,  37018,\n",
            "         35702,  26888,     90,     17,   3417,     90,     17,  31716,    271,\n",
            "         54815,     11,    279,   7192,    897,    315,  57960,   9407,   1124,\n",
            "         16827,      3,   4221,   1493,     11,  12831,    279,   6785,   2750,\n",
            "            11,    374,    510,  78045,   1124,  16827,    284,   1124,  79075,\n",
            "            90,     19,     20,  24884,  43298,     92,   1124,     60, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643],\n",
            "       device='cuda:0'), tensor([    32,   6236,   1948,    264,  22208,   1196,    323,    458,  20443,\n",
            "         11229,  17847,     13,    576,  17847,   6696,  10950,     11,  11682,\n",
            "            11,    323,  47787,  11253,    311,    279,   1196,    594,   4755,\n",
            "          3918,     82,     29,   6448,     25,  47649,  45564,   1045,   6467,\n",
            "           323,    220,     21,   9508,     13,   2932,   5927,    220,     23,\n",
            "          6467,     13,   2932,   1221,   5927,    264,   4843,    315,    279,\n",
            "          9508,    311,    279,   6733,    323,  10067,    700,    220,     24,\n",
            "           803,   6467,     13,  47649,   1431,    702,    220,     17,     15,\n",
            "          9508,    323,   6467,     13,   2585,   1657,   6467,   1521,  47649,\n",
            "         15102,  17193,     30,   6771,    594,   1744,   3019,    553,   3019,\n",
            "           323,   2550,    279,   1590,   4226,   2878,   1124,  79075,  46391,\n",
            "         35560,   3846,   2821,     25,  10061,    594,  78064,    279,   1372,\n",
            "           315,   6467,  47649,  15102,  45564,    438,    425,    382,  11190,\n",
            "           311,    279,   1995,   2661,   1447,     16,     13,  47649,  45564,\n",
            "           425,   6467,    323,    220,     21,   9508,  15102,    624,     17,\n",
            "            13,   2932,   5927,    220,     23,   6467,     11,    773,   1340,\n",
            "          1030,    425,    481,    220,     23,   6467,   2115,    624,     18,\n",
            "            13,   2932,   5927,    264,   4843,    315,    279,   9508,     11,\n",
            "           892,    374,    220,     21,    608,    220,     18,    284,    220,\n",
            "            17,   9508,     11,    773,   1340,   1030,    220,     21,    481,\n",
            "           220,     17,    284,    220,     19,   9508,   2115,    624,     19,\n",
            "            13,   2932,  10067,    700,    220,     24,    803,   6467,     11,\n",
            "           773,   1340,   1030,    425,    481,    220,     23,    488,    220,\n",
            "            24,    284,    425,    488,    220,     16,   6467,    382,   7039,\n",
            "            11,    582,   1414,    429,  47649,    702,    264,   2790,    315,\n",
            "           220,     17,     15,   9508,    323,   6467,  10856,     13,   1096,\n",
            "          3363,   1447,     33,    488,    220,     16,    320,  12110,      8,\n",
            "           488,    220,     19,    320,  39420,      8,    284,    220,     17,\n",
            "            15,    271,   7039,     11,   1077,    594,  11625,    369,    425,\n",
            "          1447,     33,    488,    220,     16,    488,    220,     19,    284,\n",
            "           220,     17,     15,    198,     33,    488,    220,     20,    284,\n",
            "           220,     17,     15,    198,     33,    284,    220,     17,     15,\n",
            "           481,    220,     20,    198,     33,    284,    220,     16,     20,\n",
            "           271,     50,    361,  15102,  45564,  57960,  79075,     90,     16,\n",
            "            20,  31716,    220,   6467,     13, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643],\n",
            "       device='cuda:0')], \n",
            "labels:\n",
            "[tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100, 100345, 103929,\n",
            "         53481, 100141,  85106, 107878,  37029,   3837,  85106,  88991, 105372,\n",
            "        100649, 102100, 104361, 105373, 118406,   3837, 104406,  17447, 100634,\n",
            "        118287,   3837,  86744, 105251,  86744, 104460, 104579,   1773, 151643,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100],\n",
            "       device='cuda:0'), tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          5338,     11,    582,  18130,    279,  23606,   2097,   5503,    304,\n",
            "           264,   4428,  11153,    510,  78045,    320,     89,     61,     17,\n",
            "           481,    220,     16,   2376,     89,     61,     21,    481,   1147,\n",
            "            61,     19,    488,   1147,     61,     17,    481,    220,     16,\n",
            "             8,    284,   1147,     61,     23,    481,    220,     16,    284,\n",
            "           220,     15,     13,   1124,    921,   1986,  23945,    198,  78045,\n",
            "          1147,     61,     23,    284,    220,     16,   1124,  26243,   1147,\n",
            "           284,   1124,   7884,    606,     90,  78055,     92,   1124,   2359,\n",
            "         11520,  37018,     90,     18,     21,     15,  24884,  43298,   1124,\n",
            "         50853,    595,  15170,     23,  11035,   1291,      8,    284,   1124,\n",
            "          7884,    606,     90,  78055,     92,    320,     19,     20,  24884,\n",
            "         43298,   1124,  50853,    595,      8,   1124,    921,   1958,   1045,\n",
            "          7546,    400,     74,  54876,    448,    400,     74,    284,    220,\n",
            "            15,     11,    220,     16,     11,    220,     17,     11,  60353,\n",
            "           220,     22,      3,    382,  11209,     11,    311,   1477,    279,\n",
            "         19703,    315,    279,   4024,  10807,  23606,     11,    582,   1083,\n",
            "          1184,    311,  21687,  19703,    315,    400,     89,     61,     17,\n",
            "           284,    220,     16,  54876,    600,   1734,   2572,    400,     89,\n",
            "           284,   1124,   5187,     16,  12947,   1096,  41208,  11508,    311,\n",
            "          1447,     12,  57960,   7884,    606,     90,  78055,     92,    220,\n",
            "            19,     20,  24884,  43298,  25046,     12,  57960,   7884,    606,\n",
            "            90,  78055,     92,    220,     16,     18,     20,  24884,  43298,\n",
            "         25046,     12,  57960,   7884,    606,     90,  78055,     92,    220,\n",
            "            17,     17,     20,  24884,  43298,  25046,     12,  57960,   7884,\n",
            "           606,     90,  78055,     92,    220,     18,     16,     20,  24884,\n",
            "         43298,  66426,  12549,    582,    525,   8014,    304,    279,   7192,\n",
            "         49952,    949,     11,  15251,    438,  57960,   9407,   1124,  16827,\n",
            "         54876,    582,  15442,    279,  75259,   2750,    518,   1493,   3501,\n",
            "           510,     12,  57960,   9407,      7,     19,     20,  24884,  43298,\n",
            "             8,    284,   1124,  37018,  35702,  26888,     90,     17,   3417,\n",
            "            90,     17,     92,  25046,     12,  57960,   9407,      7,     16,\n",
            "            18,     20,  24884,  43298,      8,    284,    481,     59,  37018,\n",
            "         35702,  26888,     90,     17,   3417,     90,     17,     92,  25046,\n",
            "            12,  57960,   9407,      7,     17,     17,     20,  24884,  43298,\n",
            "             8,    284,    481,     59,  37018,  35702,  26888,     90,     17,\n",
            "          3417,     90,     17,     92,  25046,     12,  57960,   9407,      7,\n",
            "            18,     16,     20,  24884,  43298,      8,    284,   1124,  37018,\n",
            "         35702,  26888,     90,     17,   3417,     90,     17,  31716,    271,\n",
            "         54815,     11,    279,   7192,    897,    315,  57960,   9407,   1124,\n",
            "         16827,      3,   4221,   1493,     11,  12831,    279,   6785,   2750,\n",
            "            11,    374,    510,  78045,   1124,  16827,    284,   1124,  79075,\n",
            "            90,     19,     20,  24884,  43298,     92,   1124,     60, 151643,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100],\n",
            "       device='cuda:0'), tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,  10061,    594,  78064,    279,   1372,\n",
            "           315,   6467,  47649,  15102,  45564,    438,    425,    382,  11190,\n",
            "           311,    279,   1995,   2661,   1447,     16,     13,  47649,  45564,\n",
            "           425,   6467,    323,    220,     21,   9508,  15102,    624,     17,\n",
            "            13,   2932,   5927,    220,     23,   6467,     11,    773,   1340,\n",
            "          1030,    425,    481,    220,     23,   6467,   2115,    624,     18,\n",
            "            13,   2932,   5927,    264,   4843,    315,    279,   9508,     11,\n",
            "           892,    374,    220,     21,    608,    220,     18,    284,    220,\n",
            "            17,   9508,     11,    773,   1340,   1030,    220,     21,    481,\n",
            "           220,     17,    284,    220,     19,   9508,   2115,    624,     19,\n",
            "            13,   2932,  10067,    700,    220,     24,    803,   6467,     11,\n",
            "           773,   1340,   1030,    425,    481,    220,     23,    488,    220,\n",
            "            24,    284,    425,    488,    220,     16,   6467,    382,   7039,\n",
            "            11,    582,   1414,    429,  47649,    702,    264,   2790,    315,\n",
            "           220,     17,     15,   9508,    323,   6467,  10856,     13,   1096,\n",
            "          3363,   1447,     33,    488,    220,     16,    320,  12110,      8,\n",
            "           488,    220,     19,    320,  39420,      8,    284,    220,     17,\n",
            "            15,    271,   7039,     11,   1077,    594,  11625,    369,    425,\n",
            "          1447,     33,    488,    220,     16,    488,    220,     19,    284,\n",
            "           220,     17,     15,    198,     33,    488,    220,     20,    284,\n",
            "           220,     17,     15,    198,     33,    284,    220,     17,     15,\n",
            "           481,    220,     20,    198,     33,    284,    220,     16,     20,\n",
            "           271,     50,    361,  15102,  45564,  57960,  79075,     90,     16,\n",
            "            20,  31716,    220,   6467,     13, 151643,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100],\n",
            "       device='cuda:0')]\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:09.369\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m878\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]:\n",
            "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 达芙通与黄体酮针能一起用吗？，无 ASSISTANT:根据你的描述一般需要分开使用，需要正确对待增加营养补充维生素微量元素，定期上医院复查，易消化易吸收饮食。<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\u001b[0m\n",
            "\u001b[32m2025-12-15 04:29:09.404\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m881\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]:\n",
            "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>根据你的描述一般需要分开使用，需要正确对待增加营养补充维生素微量元素，定期上医院复查，易消化易吸收饮食。<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\u001b[0m\n",
            "{'loss': 0.9113, 'grad_norm': 0.9173002243041992, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}\n",
            "{'loss': 2.1409, 'grad_norm': 1.432938814163208, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.04}\n",
            "{'loss': 2.0182, 'grad_norm': 1.1938329935073853, 'learning_rate': 1.940677966101695e-05, 'epoch': 0.08}\n",
            "{'loss': 1.7879, 'grad_norm': 1.0955268144607544, 'learning_rate': 1.8559322033898307e-05, 'epoch': 0.12}\n",
            "{'loss': 1.5086, 'grad_norm': 1.3007001876831055, 'learning_rate': 1.7711864406779662e-05, 'epoch': 0.16}\n",
            "{'loss': 1.7552, 'grad_norm': 1.177106499671936, 'learning_rate': 1.6864406779661018e-05, 'epoch': 0.2}\n",
            " 20% 50/249 [00:16<01:04,  3.09it/s]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.5514583587646484, 'eval_runtime': 0.2361, 'eval_samples_per_second': 42.356, 'eval_steps_per_second': 12.707, 'epoch': 0.2}\n",
            " 20% 50/249 [00:17<01:04,  3.09it/s]\n",
            "100% 3/3 [00:00<00:00, 19.72it/s]\u001b[A\n",
            "{'loss': 1.758, 'grad_norm': 1.1360214948654175, 'learning_rate': 1.6016949152542373e-05, 'epoch': 0.24}\n",
            "{'loss': 1.4293, 'grad_norm': 0.7010952234268188, 'learning_rate': 1.5169491525423729e-05, 'epoch': 0.28}\n",
            "{'loss': 1.7806, 'grad_norm': 1.5315754413604736, 'learning_rate': 1.4322033898305086e-05, 'epoch': 0.32}\n",
            "{'loss': 1.8093, 'grad_norm': 1.138351321220398, 'learning_rate': 1.3474576271186442e-05, 'epoch': 0.36}\n",
            "{'loss': 1.8765, 'grad_norm': 1.0683931112289429, 'learning_rate': 1.2627118644067797e-05, 'epoch': 0.4}\n",
            " 40% 100/249 [00:33<00:48,  3.08it/s]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.5318365097045898, 'eval_runtime': 0.2417, 'eval_samples_per_second': 41.37, 'eval_steps_per_second': 12.411, 'epoch': 0.4}\n",
            " 40% 100/249 [00:33<00:48,  3.08it/s]\n",
            "100% 3/3 [00:00<00:00, 19.28it/s]\u001b[A\n",
            "{'loss': 1.8634, 'grad_norm': 1.3348731994628906, 'learning_rate': 1.1779661016949153e-05, 'epoch': 0.44}\n",
            "{'loss': 1.3098, 'grad_norm': 0.9509788155555725, 'learning_rate': 1.0932203389830509e-05, 'epoch': 0.48}\n",
            "{'loss': 1.3443, 'grad_norm': 0.9731731414794922, 'learning_rate': 1.0084745762711864e-05, 'epoch': 0.52}\n",
            "{'loss': 1.8259, 'grad_norm': 0.9811198711395264, 'learning_rate': 9.237288135593222e-06, 'epoch': 0.56}\n",
            "{'loss': 1.5239, 'grad_norm': 1.5595322847366333, 'learning_rate': 8.389830508474577e-06, 'epoch': 0.6}\n",
            " 60% 150/249 [00:49<00:32,  3.08it/s]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.5247970819473267, 'eval_runtime': 0.2477, 'eval_samples_per_second': 40.371, 'eval_steps_per_second': 12.111, 'epoch': 0.6}\n",
            " 60% 150/249 [00:49<00:32,  3.08it/s]\n",
            "100% 3/3 [00:00<00:00, 18.72it/s]\u001b[A\n",
            "{'loss': 1.9246, 'grad_norm': 1.2348332405090332, 'learning_rate': 7.542372881355933e-06, 'epoch': 0.64}\n",
            "{'loss': 1.8667, 'grad_norm': 1.055569052696228, 'learning_rate': 6.694915254237288e-06, 'epoch': 0.68}\n",
            "{'loss': 1.6389, 'grad_norm': 0.8694339990615845, 'learning_rate': 5.847457627118645e-06, 'epoch': 0.72}\n",
            "{'loss': 1.7602, 'grad_norm': 1.3636503219604492, 'learning_rate': 5e-06, 'epoch': 0.76}\n",
            "{'loss': 1.6602, 'grad_norm': 1.0272732973098755, 'learning_rate': 4.152542372881356e-06, 'epoch': 0.8}\n",
            " 80% 200/249 [01:05<00:15,  3.18it/s]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.5217380523681641, 'eval_runtime': 0.2423, 'eval_samples_per_second': 41.276, 'eval_steps_per_second': 12.383, 'epoch': 0.8}\n",
            " 80% 200/249 [01:06<00:15,  3.18it/s]\n",
            "100% 3/3 [00:00<00:00, 18.92it/s]\u001b[A\n",
            "{'loss': 1.9244, 'grad_norm': 1.6173402070999146, 'learning_rate': 3.305084745762712e-06, 'epoch': 0.84}\n",
            "{'loss': 1.4625, 'grad_norm': 1.6691126823425293, 'learning_rate': 2.457627118644068e-06, 'epoch': 0.88}\n",
            "{'loss': 1.5878, 'grad_norm': 1.0700232982635498, 'learning_rate': 1.6101694915254237e-06, 'epoch': 0.92}\n",
            "{'loss': 1.6469, 'grad_norm': 0.8789297938346863, 'learning_rate': 7.627118644067798e-07, 'epoch': 0.96}\n",
            "{'train_runtime': 81.9997, 'train_samples_per_second': 12.134, 'train_steps_per_second': 3.037, 'train_loss': 1.712254544338548, 'epoch': 1.0}\n",
            "100% 249/249 [01:21<00:00,  3.04it/s]\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               =   954370GF\n",
            "  train_loss               =     1.7123\n",
            "  train_runtime            = 0:01:21.99\n",
            "  train_samples            =       1000\n",
            "  train_samples_per_second =     12.134\n",
            "  train_steps_per_second   =      3.037\n",
            "\u001b[32m2025-12-15 04:30:31.899\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m898\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 81.9997, 'train_samples_per_second': 12.134, 'train_steps_per_second': 3.037, 'total_flos': 1024747531133952.0, 'train_loss': 1.712254544338548, 'epoch': 1.0, 'train_samples': 1000}\u001b[0m\n",
            "\u001b[32m2025-12-15 04:30:31.899\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m899\u001b[0m - \u001b[1mSaving model checkpoint to outputs-sft-v1\u001b[0m\n",
            "\u001b[32m2025-12-15 04:30:32.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m908\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
            "100% 3/3 [00:00<00:00, 19.50it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_loss               =     0.5222\n",
            "  eval_runtime            = 0:00:00.24\n",
            "  eval_samples            =         10\n",
            "  eval_samples_per_second =     41.256\n",
            "  eval_steps_per_second   =     12.377\n",
            "  perplexity              =     1.6857\n",
            "\u001b[32m2025-12-15 04:30:32.632\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m921\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 0.5221967101097107, 'eval_runtime': 0.2424, 'eval_samples_per_second': 41.256, 'eval_steps_per_second': 12.377, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 1.685726638156997}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python supervised_finetuning.py \\\n",
        "    --model_name_or_path merged-pt \\\n",
        "    --train_file_dir ./data/finetune \\\n",
        "    --validation_file_dir ./data/finetune \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.05 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --eval_strategy steps \\\n",
        "    --save_steps 500 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --output_dir outputs-sft-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHLbKzmlFndL",
        "outputId": "77594323-e852-4033-fbe5-e89ee00dafdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 22M\n",
            "-rw-r--r-- 1 root root  713 Dec 15 04:30 adapter_config.json\n",
            "-rw-r--r-- 1 root root  17M Dec 15 04:30 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root  605 Dec 15 04:30 added_tokens.json\n",
            "-rw-r--r-- 1 root root  431 Dec 15 04:30 all_results.json\n",
            "drwxr-xr-x 2 root root 4.0K Dec 15 04:30 \u001b[0m\u001b[01;34mcheckpoint-249\u001b[0m/\n",
            "-rw-r--r-- 1 root root  222 Dec 15 04:30 eval_results.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 15 04:30 merges.txt\n",
            "-rw-r--r-- 1 root root 5.0K Dec 15 04:30 README.md\n",
            "drwxr-xr-x 3 root root 4.0K Dec 15 04:29 \u001b[01;34mruns\u001b[0m/\n",
            "-rw-r--r-- 1 root root  648 Dec 15 04:30 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 7.1K Dec 15 04:30 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 6.1K Dec 15 04:30 trainer_state.json\n",
            "-rw-r--r-- 1 root root  229 Dec 15 04:30 train_results.json\n",
            "-rw-r--r-- 1 root root 3.3M Dec 15 04:30 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh outputs-sft-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "mDrNSDb3FndL"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "4S9q05QXFndL"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfzxSdsGFndL",
        "outputId": "7b661aae-30ca-43b3-d9fb-36bcef00f116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-15 04:31:06.273614: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-15 04:31:06.290849: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765773066.312558    6875 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765773066.319108    6875 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765773066.335694    6875 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765773066.335734    6875 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765773066.335736    6875 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765773066.335739    6875 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-15 04:31:06.340717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Namespace(base_model='merged-pt', tokenizer_path=None, lora_model='outputs-sft-v1', resize_emb=False, output_dir='./merged-sft', hf_hub_model_id='', hf_hub_token=None)\n",
            "Base model: merged-pt\n",
            "LoRA model: outputs-sft-v1\n",
            "Loading LoRA for causal language model\n",
            "Merging with merge_and_unload...\n",
            "Saving to Hugging Face format...\n",
            "Done! model saved to ./merged-sft\n"
          ]
        }
      ],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir ./merged-sft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTRw_dM6FndL",
        "outputId": "7879005e-c4e7-49a5-9c26-bea2d8cf9318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 958M\n",
            "-rw-r--r-- 1 root root  605 Dec 15 04:31 added_tokens.json\n",
            "-rw-r--r-- 1 root root  736 Dec 15 04:31 config.json\n",
            "-rw-r--r-- 1 root root  117 Dec 15 04:31 generation_config.json\n",
            "-rw-r--r-- 1 root root 1.6M Dec 15 04:31 merges.txt\n",
            "-rw-r--r-- 1 root root 943M Dec 15 04:31 model.safetensors\n",
            "-rw-r--r-- 1 root root  616 Dec 15 04:31 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 7.1K Dec 15 04:31 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Dec 15 04:31 tokenizer.json\n",
            "-rw-r--r-- 1 root root 2.7M Dec 15 04:31 vocab.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh merged-sft/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiV48NBpFndL",
        "outputId": "ee7c651d-7421-401e-da00-b3b12f9cd0ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"_name_or_path\": \"merged-pt\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "%cat merged-sft/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ns2JzzvuFndL"
      },
      "source": [
        "Stage2 SFT训练完成。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qEhCi52mFndM"
      },
      "source": [
        "# Stage 3: DPO(Direct Preference Optimization)\n",
        "\n",
        "第三阶段：DPO(Direct Preference Optimization)直接偏好优化，DPO通过直接优化语言模型来实现对其行为的精确控制，而无需使用复杂的强化学习，也可以有效学习到人类偏好，DPO相较于RLHF更容易实现且易于训练，效果更好\n",
        "\n",
        "| Stage 3: Direct Preference Optimization        |  [dpo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/dpo_training.py) | [run_dpo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_dpo.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "wGlLOcQkFndM"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是`Qwen/Qwen2.5-0.5B` 或者 Stage2得到的SFT模型\n",
        "2. 数据集：DPO阶段使用的是医疗reward数据，抽样了500条，位于`data/reward`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "X6wbuu6JFndM"
      },
      "source": [
        "## Stage3 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njSwiortFndM",
        "outputId": "4e41b3e9-7ed4-4fc1-e735-e2b2ef66009f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dpo_zh_500.jsonl\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/reward/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5X1glk1FndM",
        "outputId": "88c7dfe1-a1e0-40d6-cef1-72d9b095e1c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-15 04:31:50.587736: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-15 04:31:50.604859: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765773110.625802    7113 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765773110.632175    7113 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765773110.648324    7113 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765773110.648352    7113 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765773110.648355    7113 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765773110.648358    7113 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-15 04:31:50.653126: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MedicalGPT/dpo_training.py\", line 25, in <module>\n",
            "    from trl import DPOTrainer, DPOConfig\n",
            "ImportError: cannot import name 'DPOConfig' from 'trl' (/usr/local/lib/python3.12/dist-packages/trl/__init__.py). Did you mean: 'CPOConfig'?\n"
          ]
        }
      ],
      "source": [
        "!python dpo_training.py \\\n",
        "    --model_name_or_path ./merged-sft \\\n",
        "    --template_name qwen \\\n",
        "    --train_file_dir ./data/reward \\\n",
        "    --validation_file_dir ./data/reward \\\n",
        "    --per_device_train_batch_size 3 \\\n",
        "    --per_device_eval_batch_size 1 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 500 \\\n",
        "    --max_steps 100 \\\n",
        "    --eval_steps 10 \\\n",
        "    --save_steps 50 \\\n",
        "    --max_source_length 256 \\\n",
        "    --max_target_length 256 \\\n",
        "    --output_dir outputs-dpo-v1 \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --bf16 True \\\n",
        "    --fp16 False \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --remove_unused_columns False \\\n",
        "    --gradient_checkpointing True \\\n",
        "    --cache_dir ./cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_KDADSPFndM",
        "outputId": "86beb736-c715-4d79-8697-c142399f51e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'outputs-dpo-v1': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "%ls -lh outputs-dpo-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dE078rsYFndM"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "61dLGiFXFndM"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-RmOn_iFndN"
      },
      "outputs": [],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model merged-sft --lora_model outputs-dpo-v1 --output_dir merged-dpo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwE0Wi4eFndN"
      },
      "outputs": [],
      "source": [
        "%ls -lh merged-dpo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pszVUf0WFndN"
      },
      "outputs": [],
      "source": [
        "%cat merged-dpo/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Na7W2Ii3FndN"
      },
      "source": [
        "Stage3 偏好建模第一次训练完成。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "GVRdsI_TFndN"
      },
      "source": [
        "**至此一个完整的训练流程演示完成。**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-26T12:34:29.658428Z",
          "start_time": "2023-06-26T12:34:29.620609Z"
        },
        "id": "Gwn8PsebFndO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "i4qPrRf5FndO"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-26T12:35:00.864463Z",
          "start_time": "2023-06-26T12:34:47.802087Z"
        },
        "id": "sACJt1wkFndO"
      },
      "outputs": [],
      "source": [
        "!python inference.py --base_model merged-dpo\n",
        "# 或在shell中运行\n",
        "# python inference.py --base_model merged-dpo --interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "d-rYsUyFFndO"
      },
      "source": [
        "Input:介绍下南京\n",
        "Response:  南京市位于江苏省西南部，是全国首批历史文化名城、国家中心城市和自由贸易试验区。\n",
        "\n",
        "完。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeUmw6xEFndO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
      }
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}